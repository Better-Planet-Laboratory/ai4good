{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering with Agricultural Commoditites\n",
    "\n",
    "This week, we focus on the concept of embeddings and look at two very machine learning methods that use embeddings: collaborative filtering and autoencoders.  Embeddings are how machine learning models represent data in a structured way, largely through reducing high dimensional data into continuous vectors.\n",
    "\n",
    "![image](https://gisgeography.com/wp-content/uploads/2014/12/feed-world-agriculture-map.png)\n",
    "\n",
    "When you are shopping online or watching content on streaming services, have you ever thought about how other items or shows are recommended to you? These suggestions are largely due to Recommender systems, which are a way of suggesting similar content to a user's specific preferences.  A common method in Recommender systems is Collaborative Filtering, which recommends content based on similarity measures between users and the content.  In Collaborative Filtering a simple approach is to rely on matrix factorization to identify the relationships between items and user entities. Latent features, the association between users and item matrices, are determine to find similarity and make predictions. Common examples of Collaborative Filtering include the movie or song/artist recommendations from the MovieLens or lastfm datasets, examples of which you can find online.\n",
    "\n",
    "\n",
    "For this workbook, we will be focusing on a dataset which features 50+ agricultural commodities grown in over 3200+ administrative units around the world. This data is implicit so we only know if a commodity is grown or not in each location. Imagine you are a policymaker in the agriculture sector and you want to improve nutritional and food security outcomes by improving crop diversity, that is, increasing the number of commodities grown in all the administrative units around the world.  To make these recommendations you would need to understand context-specific information on what crops grow best where and what part of world can grow different crops, but this might be information that you can't easily access.  Instead, for an initial understanding we can use our data to build a recommender system to answer the following questions:\n",
    "1. What commodities are most similar to one another?\n",
    "2. What admin units are most similar to one another?\n",
    "3. What are the top 5 new commodities that we could recommend for an admin unit to grow?\n",
    "\n",
    "\n",
    "By the end of this exercise we  will have developed 3 outputs: A similarity matrix of commodities, a similarity matrix of admin units and lastly, a recommendation of the top 5 new commodities for each admin unit in our dataset.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand how to create a similarity matrix from a pairwise dataset\n",
    "2. Understand how to use matrix factorization to create a recommender system\n",
    "3. Interpret the results of a recommender system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop-Crop Similarities\n",
    "Here, we'll aim to understand what crops are similar to each other based on whether they are grown in the same regions as each other. Let's start by setting up our environment and loading in our data. Ensure that you have the [corresponding dataset](https://kaggle.com/datasets/18185cb6db270f05eee91f017174ca33fefd087dcdf99ae15f87b675587e2af0) pulled into this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:43:16.181635Z",
     "start_time": "2025-01-07T17:43:16.176698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:43:18.251975Z",
     "start_time": "2025-01-07T17:43:18.218515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_path = '/kaggle/input/commodotiesgrown-byadmin1unit'\n",
    "filename = 'commoditiesgrown_byadmin1unit.csv'\n",
    "data = pd.read_csv(os.path.join(base_path, filename))  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have the admin unit id (mostly provinces or states within countries eg. AFG.1_1 is the Badakhshan province in Afghanistan), the name of the commodity (this includes from crops and animals) as well as whether they are grown in that unit (binary, 1 for yes or 0 for no).\n",
    "\n",
    "Lets pivot the data and create a matrix. We will also zero any NAs and index based on admin units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:43:24.878548Z",
     "start_time": "2025-01-07T17:43:24.823564Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DfMatrix = pd.pivot_table(data, values='grown', index='admin_id', columns='commodity')\n",
    "DfMatrix=DfMatrix.fillna(0)\n",
    "DfResetted = DfMatrix.reset_index().rename_axis(None, axis=1)\n",
    "DfResetted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first task which is to develop a commodity-commodity similarity matrix we will drop admin units and normalize our data for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:43:37.894416Z",
     "start_time": "2025-01-07T17:43:37.875483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfCommodities = DfResetted.drop('admin_id', axis=1) \n",
    "dfCommoditiesNorm = dfCommodities/np.sqrt(np.square(dfCommodities).sum(axis=0))   \n",
    "dfCommoditiesNorm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the cosine similarity for all our commodities.  To get the cosine similarity we will take the dot product (matrix multiplication) of normalized dataset (3278x54) by its transpose (54x3278) which will result in a 55x55 matrix of the cosine similarities of all commodity combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:43:55.388223Z",
     "start_time": "2025-01-07T17:43:55.374927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ItemItemSim = dfCommoditiesNorm.transpose().dot(dfCommoditiesNorm) \n",
    "ItemItemSim.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cosine similarity function from sklearn we get the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:43:59.339652Z",
     "start_time": "2025-01-07T17:43:59.309873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_cosine=pd.DataFrame(cosine_similarity(dfCommoditiesNorm.transpose(),dense_output=True))\n",
    "df_cosine.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the top 10 most similar commodities for each, do the recommendations make sense? From what you know, are these crops often grown together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:44:01.800560Z",
     "start_time": "2025-01-07T17:44:01.785336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ItemTop10 = pd.DataFrame(index=ItemItemSim.columns,columns=range(1,11))\n",
    "for i in range(0,len(ItemItemSim.columns)): ItemTop10.iloc[i,:10] = ItemItemSim.iloc[0:,i].sort_values(ascending=False)[:10].index  \n",
    "ItemTop10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to make sense! We're seeing tropical commodities most similar to other tropical commodities, different meats most alike etc. \n",
    "Write the top 10 commodity-commodity similarities to a file. You should find it in \"Output\" under `/kaggle/working/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:44:07.067772Z",
     "start_time": "2025-01-07T17:44:07.048173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# export to excel\n",
    "ItemTop10.to_excel(\"commodity_commodity_top10.xlsx\")\n",
    "print(\"file written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a gut check on our commodities with a Principal Component Analysis (PCA) which is helps with dimensionality reduction.  Let's look at how our commodity variable is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:44:16.839548Z",
     "start_time": "2025-01-07T17:44:16.418010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_tranposed=dfCommoditiesNorm.transpose()\n",
    "df_tranposed.index.name = 'label'\n",
    "df_tranposed.reset_index(inplace=True)\n",
    "\n",
    "pca_comm = PCA(n_components=2)\n",
    "principalComponents_comm = pca_comm.fit_transform(df_tranposed.iloc[:,1:])\n",
    "principal_comm_Df = pd.DataFrame(data = principalComponents_comm\n",
    "             , columns = ['pc1', 'pc2'])\n",
    "principal_comm_Df['label'] = df_tranposed['label']\n",
    "principal_comm_Df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:44:18.881040Z",
     "start_time": "2025-01-07T17:44:18.732630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot pc1 and pc2\n",
    "plt.figure(figsize=(16,8))\n",
    "# Normalize values for color scaling\n",
    "pc1_norm = (principal_comm_Df[\"pc1\"] - principal_comm_Df[\"pc1\"].min()) / (principal_comm_Df[\"pc1\"].max() - principal_comm_Df[\"pc1\"].min())\n",
    "pc2_norm = (principal_comm_Df[\"pc2\"] - principal_comm_Df[\"pc2\"].min()) / (principal_comm_Df[\"pc2\"].max() - principal_comm_Df[\"pc2\"].min())\n",
    "\n",
    "# Create blended colors\n",
    "colors = np.column_stack([pc1_norm, pc2_norm, np.ones_like(pc1_norm)])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"pc1\", y=\"pc2\",\n",
    "    data=principal_comm_Df,\n",
    "    s=200,  # Increase dot size\n",
    "    color=colors  # Apply blended colors\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "for i in range(principal_comm_Df.shape[0]):\n",
    "    plt.text(x=principal_comm_Df.pc1[i] + 0.005, y=principal_comm_Df.pc2[i] + 0.005,\n",
    "             s=principal_comm_Df.label[i], fontdict=dict(size=10))\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")  # x label\n",
    "plt.ylabel(\"Principal Component 2\")  # y label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this PCA that similar commodities are clustering together (meats, tropical crops such as coconuts/coffee/pineapples etc.).  This is also reflected in our commodity cosine similarity index we have generated. What other interesting clusters can you find in this PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin-Admin Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, lets generate the cosine similarities of which admin units are most alike to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T18:41:57.744880Z",
     "start_time": "2025-01-07T18:41:57.026568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DfMatrix2 = pd.pivot_table(data, values='grown', columns='admin_id', index='commodity')\n",
    "DfMatrix2 = DfMatrix2.fillna(0)\n",
    "DfResetted2 = DfMatrix2.reset_index().rename_axis(None, axis=1)\n",
    "dfAdmins = DfResetted2.drop('commodity', axis=1) \n",
    "dfAdminsNorm = dfAdmins/np.sqrt(np.square(dfAdmins).sum(axis=0))  \n",
    "AdminAdminSim = dfAdminsNorm.transpose().dot(dfAdminsNorm) \n",
    "AdminTop10 = pd.DataFrame(index=AdminAdminSim.columns,columns=range(1,11))\n",
    "for i in range(0,len(AdminAdminSim.columns)): AdminTop10.iloc[i,:10] = AdminAdminSim.iloc[0:,i].sort_values(ascending=False)[:10].index  \n",
    "AdminTop10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial look through data shows that admin units within a country seem to be the most similar to each other.  Digging into the table, are there other interesting occurences?  Do similar types of countries (eg. geographically near, similar climates) appear? \n",
    "Write the top 10 admin-admin similarities to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:44:59.783623Z",
     "start_time": "2025-01-07T17:44:59.083500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AdminTop10.to_excel(\"admin_admin_top10.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the admin unit embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:45:01.029960Z",
     "start_time": "2025-01-07T17:45:01.010006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_tranposed2=dfAdminsNorm.transpose()\n",
    "df_tranposed2.index.name = 'label'\n",
    "df_tranposed2.reset_index(inplace=True)\n",
    "\n",
    "pca_admin = PCA(n_components=2)\n",
    "principalComponents_admin = pca_admin.fit_transform(df_tranposed2.iloc[:,1:])\n",
    "principal_admin_Df = pd.DataFrame(data = principalComponents_admin\n",
    "             , columns = ['pc1', 'pc2'])\n",
    "principal_admin_Df['label'] = df_tranposed2['label']\n",
    "principal_admin_Df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:45:06.147101Z",
     "start_time": "2025-01-07T17:45:04.296348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plot the principal components\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "# Normalize values for color scaling\n",
    "pc1_norm = (principal_admin_Df[\"pc1\"] - principal_admin_Df[\"pc1\"].min()) / (principal_admin_Df[\"pc1\"].max() - principal_admin_Df[\"pc1\"].min())\n",
    "pc2_norm = (principal_admin_Df[\"pc2\"] - principal_admin_Df[\"pc2\"].min()) / (principal_admin_Df[\"pc2\"].max() - principal_admin_Df[\"pc2\"].min())\n",
    "\n",
    "# Create blended colors\n",
    "colors = np.column_stack([pc1_norm, pc2_norm, np.ones_like(pc1_norm)])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"pc1\", y=\"pc2\",\n",
    "    data=principal_admin_Df,\n",
    "    s=100,  # Increase dot size\n",
    "    color=colors  # Apply blended colors\n",
    ")\n",
    "\n",
    "# Add text labels\n",
    "for i in range(principal_admin_Df.shape[0]):\n",
    "    plt.text(x=principal_admin_Df.pc1[i] + 0.005, y=principal_admin_Df.pc2[i] + 0.005,\n",
    "             s=principal_admin_Df.label[i], fontdict=dict(size=6))\n",
    "\n",
    "plt.xlabel(\"Principal Component 1\")  # x label\n",
    "plt.ylabel(\"Principal Component 2\")  # y label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin-Commodity Recommendations\n",
    "Now that we have a sense of what commodities are similar to each other, and what regions are similar to each other, our last task is our recommender system that suggests which commodities regions could consider growing.  We will need the commodity similarity matrix, then will look at which commodities the admin unit grows and get the top 10 neighbours for each commodity.  Then we will calculate the admin's commodity growing history for each neighbour and calculate a similarity score for them.  In the end we will recommend 5 commodities with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:45:15.929227Z",
     "start_time": "2025-01-07T17:45:15.914703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df=DfResetted \n",
    "df=df.fillna(0) \n",
    "df.head()\n",
    "AdminItemSimilarity = pd.DataFrame(index=df.index,columns=df.columns)\n",
    "AdminItemSimilarity.iloc[:,:1] = df.iloc[:,:1] \n",
    "AdminItemSimilarity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate similarity scores through the functions we will create below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:45:57.279245Z",
     "start_time": "2025-01-07T17:45:24.593156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this might a few minutes to run...\n",
    "def getScore(history, similarities):\n",
    "    return sum(history*similarities)/sum(similarities)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for i in range(0,len(AdminItemSimilarity.index)): \n",
    "    for j in range(1,len(AdminItemSimilarity.columns)): \n",
    "        user = AdminItemSimilarity.index[i] \n",
    "        product = AdminItemSimilarity.columns[j]\n",
    "        \n",
    "        if df.loc[i][j] == 1: AdminItemSimilarity.loc[i][j] = 0 \n",
    "        else: \n",
    "            ItemTop = ItemTop10.loc[product][1:10] \n",
    "            ItemTopSimilarity = ItemItemSim.loc[product].sort_values(ascending=False)[1:10] \n",
    "            AdminGrown = dfCommodities.loc[user,ItemTop] \n",
    "            \n",
    "            AdminItemSimilarity.loc[i][j] = getScore(AdminGrown,ItemTopSimilarity)\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our newly generated recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:45:59.379263Z",
     "start_time": "2025-01-07T17:45:59.370591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AdminItemSimilarity.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this into the top 5 recommendations of new commodities for each admin unit and take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:46:04.830873Z",
     "start_time": "2025-01-07T17:46:04.423206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AdminItemRecommend = pd.DataFrame(index=AdminItemSimilarity.index, columns=['Admin_id','1','2','3','4','5']) \n",
    "AdminItemRecommend.iloc[0:,0] = AdminItemSimilarity.iloc[:,0]\n",
    "for i in range(0,len(AdminItemSimilarity.index)):\n",
    "    AdminItemRecommend.iloc[i,1:] = AdminItemSimilarity.iloc[i,1:].sort_values(ascending=False).iloc[0:5,].index.transpose()\n",
    "    \n",
    "AdminItemRecommend.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks interesting, we can see that for these Afghan provinces the recommendations mostly make sense, including for pork, which is culturally not consumed and would explain why its not a commodity grown in the country. Looking through the recommendations, what else stands out? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T17:46:10.560564Z",
     "start_time": "2025-01-07T17:46:10.255318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "AdminItemRecommend.to_excel(\"admin_comm_recommend.xlsx\")\n",
    "print('exported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "Thank you very much to [Ginni Braich](https://www.linkedin.com/in/ginni-braich-41672337/?originalSubdomain=ca) of the [Better Planet Lab](https://betterplanetlab.com/about) for conceptualizing and putting together much of this notebook.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "1. Imagine that you are in the Ministry of Agriculture of Sierra Leone (SLE) and have been tasked with diversifying your country's agricultural industry. What crops are most likely to be successful in your climate based on the results above? What other data or information would you ask for to increase your confidence in the recommendations? Are these results actionable?\n",
    "\n",
    "2. The above code uses cosine similarity to measure the \"distance\" between crops or administrative units, but there are quite a few other distance measures. Skim through [this Medium article](https://medium.com/@jodancker/a-brief-introduction-to-distance-measures-ac89cbd2298) on other mathematical distances, pick one that seems appropriate, and rerun the analysis using this distance measure instead. Does it change your results for question #1? Why or why not?\n",
    "\n",
    "3. The similarity matrix of crops is a form of \"embeddings\". Here, we used them to recommend crops to given regions of the world based on what other similar regions are growing. What is another task you could use these \"embeddings\" for?\n",
    "\n",
    "## Bonus Assignment\n",
    "1. The above example uses memory-based collaborative filtering. In the video you learned about matrix factorization optimized with stochastic gradient descent. Re-implement the example above with this alternative model. Do the results differ? Why might you choose this over the memory-based version?\n",
    "\n",
    "## Deliverables\n",
    "Write out your answers to the questions above. Make this notebook public (Save Version > Share > Public > Copy link) and share the link on Discord in the #notebook-submissions channel. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6569693,
     "sourceId": 10611904,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
