{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10846981,"sourceType":"datasetVersion","datasetId":6736497}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using Transformers to Understand Product Labelling","metadata":{}},{"cell_type":"markdown","source":"![Image](https://raw.githubusercontent.com/Better-Planet-Laboratory/climatejusticelabels/main/Supplementary%20Figures/Fig1_fin.png)","metadata":{}},{"cell_type":"markdown","source":"The consumption of many products and services carries carbon costs, and associated human health and mortality impacts due to climate change. In late 2024 we published an article proposing that accounting for these costs in labels could drive manufacturers towards net zero targets. You can read the paper here https://www.nature.com/articles/s41562-024-02087-0\n\nThis idea is controversial as you can imagine. You might wonder how people responded to this idea. While it is hard to assess directly from the paper,  earlier on in the year we did run an advocacy campaign which included a brief survey in which people had to pick some food items, travel options and from that were told how many minutes they would take from another persons life due to the climate change impacts of those consumption choices. https://ziamehrabi.medium.com/calculate-the-human-impact-of-your-everyday-decisions-6a65d63efec9. We  asked students in an Introductory Environmental Studies class what they thought too. \n\nThe responses those students created were free form text based responses to a conversation. In this notebook you'll be leveraging pre-trained and off-the-shelf fine tuned language models to try and learn what those students thought and how this idea could potentially be improved.","metadata":{}},{"cell_type":"markdown","source":"## Packages","metadata":{}},{"cell_type":"code","source":"# read and manipulate data\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport nltk\nfrom sklearn.decomposition import PCA\nfrom tqdm import tqdm  # For progress tracking\n\n# Ensure necessary NLTK data is downloaded\nnltk.download(\"punkt\")\n\n# for running llms from hugging face\nimport torch\nfrom transformers import pipeline\n\n# ignore warnings because they're annoying\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:29:32.658468Z","iopub.execute_input":"2025-02-25T20:29:32.658790Z","iopub.status.idle":"2025-02-25T20:29:53.427231Z","shell.execute_reply.started":"2025-02-25T20:29:32.658764Z","shell.execute_reply":"2025-02-25T20:29:53.425791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models\n\nWe'll be using a couple of models in this notebook, a fine-tuned version of BERT for sentiment analysis (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english), and a fine tuned version of BART for summarization (https://huggingface.co/facebook/bart-large-cnn). ","metadata":{}},{"cell_type":"markdown","source":"## Pipeline() overview\n\nWe'll be using the pipeline() function for this notebook. It is a very powerful abstracted API for running a range of standardized pipelines with the Hugging Face transformers library. You can find out more here: https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/pipelines#transformers.pipeline.\n\nBelow we demonstrate the use of the pipeline() function for sentiment analysis. Note the pipelines function will download the model locally for use. We will first test it works with a simple sequence of text, and return predicted probabilities of the sentiment.\n\n","metadata":{}},{"cell_type":"code","source":"# set up pipeline\nclassifier = pipeline(\n    task=\"sentiment-analysis\",\n    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n    device=0\n)\npreds = classifier(\"Hugging Face is the best thing since sliced bread!\")\npreds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\npreds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:30:44.265103Z","iopub.execute_input":"2025-02-25T20:30:44.265441Z","iopub.status.idle":"2025-02-25T20:30:44.553074Z","shell.execute_reply.started":"2025-02-25T20:30:44.265414Z","shell.execute_reply":"2025-02-25T20:30:44.552360Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparison to vanilla transformers\n\nThe pipeline() function is an abstraction of a number of individual tasks which you can implement with the transformers library. These include defining the model,downloading it, setting the method of tokenization, doing tokenization of input, getting predictions, converting predictions into form for interpretation, and so on. We have pasted these below so you can understand a little what is happening under the hood.\n\nSee here for quick overview of the transformers package https://huggingface.co/docs/transformers/en/quicktour","metadata":{}},{"cell_type":"code","source":"# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification #note here we are selecting specific tokenizer for this model, Autotokenizer also exists, which will automatically choose the right one for the model specified.\n\n# # Load model and tokenizer\n# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n# model = DistilBertForSequenceClassification.from_pretrained(model_name)\n\n# # Define input text\n# text = \"Hugging Face is the best thing since sliced bread!\"\n\n# # Tokenize input text\n# inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n\n# # Get predictions\n# with torch.no_grad():\n#     outputs = model(**inputs)\n#     logits = outputs.logits\n\n# # Get predicted label and score\n# predicted_class = torch.argmax(logits, dim=1).item()\n# pred_score = torch.softmax(logits, dim=1)[0, predicted_class].item()\n\n# # Map the predicted class to label\n# labels = [\"NEGATIVE\", \"POSITIVE\"]\n# pred_label = labels[predicted_class]\n\n# # Prepare output in the same format as the pipeline\n# preds = [{\"score\": round(pred_score, 4), \"label\": pred_label}]\n# print(preds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:29:57.937688Z","iopub.execute_input":"2025-02-25T20:29:57.937917Z","iopub.status.idle":"2025-02-25T20:29:57.941913Z","shell.execute_reply.started":"2025-02-25T20:29:57.937898Z","shell.execute_reply":"2025-02-25T20:29:57.940969Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Read in the data \n\nNow we have a little understanding of the APIs, we will read in the data. This is a csv with a row for each student response. There is no index column, so just use the dataframe index as the student id. There may be some blank rows and some HTML artificats in the responses (the conversations were originally recorded in HTML documents).","metadata":{}},{"cell_type":"code","source":"base_path = \"/kaggle/input/labelling/\"\nfile_name = \"student_responses.csv\"\ndf = pd.read_csv(os.path.join(base_path, file_name))\ndf.head()\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:30:48.586085Z","iopub.execute_input":"2025-02-25T20:30:48.586423Z","iopub.status.idle":"2025-02-25T20:30:48.596339Z","shell.execute_reply.started":"2025-02-25T20:30:48.586394Z","shell.execute_reply":"2025-02-25T20:30:48.595527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What did students think?\n\nHere we count the frequency of all the positive and negative responses. Overall what did people think? We'll display the first few results.","metadata":{}},{"cell_type":"code","source":"inputs = df[\"response\"].to_list()\npreds = classifier(inputs) #note we set this above when we set up the pipeline\npreds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\npreds_df = pd.DataFrame(preds)\npreds_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:30:51.288141Z","iopub.execute_input":"2025-02-25T20:30:51.288444Z","iopub.status.idle":"2025-02-25T20:30:52.416165Z","shell.execute_reply.started":"2025-02-25T20:30:51.288421Z","shell.execute_reply":"2025-02-25T20:30:52.415333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Cool. Now what did everyone think? Let's try and plot the distribution of responses.","metadata":{}},{"cell_type":"markdown","source":"What did we see overall? Looks like there was a marginally net positive reponse. Although it was aparently very mixed.","metadata":{}},{"cell_type":"code","source":"sns.histplot(\n    data=preds_df, \n    x=\"label\", \n    hue=\"label\", \n    multiple=\"stack\",\n    palette=[\"darkred\", \"green\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:29:59.178304Z","iopub.execute_input":"2025-02-25T20:29:59.178525Z","iopub.status.idle":"2025-02-25T20:29:59.443794Z","shell.execute_reply.started":"2025-02-25T20:29:59.178506Z","shell.execute_reply":"2025-02-25T20:29:59.443053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Changing sentiment\n\nIs it possible to pick out something more? One thing to note is that during the conversation students were asked three questions in sequence, How did the survey make you feel? Was it an effective way to communicate this issue? What could be improved? Below is some analysis that splits up the responses into sentences, identified the sentiment in each, and then plots the result across a normalized conversation time axis.\n","metadata":{}},{"cell_type":"code","source":"\n# Function to process each response\ndef process_response(response_text):\n    sentences = nltk.sent_tokenize(response_text)  # Split into sentences\n    num_sentences = len(sentences)\n\n    # Process sentences in batches (classifier using GPU)\n    results = classifier(sentences)\n\n    # Extract sentiment and confidence\n    sentiment_scores = [1 if r[\"label\"] == \"POSITIVE\" else -1 for r in results]\n    confidences = [r[\"score\"] for r in results]\n\n    # Normalize sentence positions (0 to 1)\n    normalized_time = np.linspace(0, 1, num_sentences)\n\n    # Return DataFrame\n    return pd.DataFrame({\"normalized_time\": normalized_time, \n                         \"sentiment\": sentiment_scores, \n                         \"confidence\": confidences, \n                         \"response_id\": response_text[:30]})  # First 30 chars as ID\n\n# Example data (Replace with your actual DataFrame `df` containing responses)\n# df = pd.DataFrame({\"response\": [\"Example response text.\", \"Another response.\"]})\n\n# Apply processing to all responses\nall_sentences_df = pd.concat(df[\"response\"].apply(process_response).tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:31:21.679087Z","iopub.execute_input":"2025-02-25T20:31:21.679401Z","iopub.status.idle":"2025-02-25T20:31:25.747556Z","shell.execute_reply.started":"2025-02-25T20:31:21.679378Z","shell.execute_reply":"2025-02-25T20:31:25.746732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we plot the results. Remember the conversation had three questions: how did the survey make you feel? Was it an effective way to communicate this issue? What could be improved?  There is a lot of noise but we also find three peaks in the responses. And there was a tendency for the sentiment on the middle peak (which we may assume to match to the question \"Was it an effective way to communicate this issue?\" to be mostly positive.","metadata":{}},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n# Split the data into positive and negative sentiments\npositive_sentiments = all_sentences_df[all_sentences_df['sentiment'] == 1]\nnegative_sentiments = all_sentences_df[all_sentences_df['sentiment'] == -1]\n\n# Plot KDE for positive and negative sentiment densities\nplt.figure(figsize=(10, 5))\n\n# KDE for positive sentiments (blue)\nsns.kdeplot(positive_sentiments['normalized_time'], \n            color='blue', \n            label='Positive Sentiment', \n            fill=True, \n            alpha=0.5,  # Transparency to visualize overlap\n            bw_adjust=0.5)  # Bandwidth adjustment for smoothness\n\n# KDE for negative sentiments (red)\nsns.kdeplot(negative_sentiments['normalized_time'], \n            color='red', \n            label='Negative Sentiment', \n            fill=True, \n            alpha=0.5,  # Transparency to visualize overlap\n            bw_adjust=0.5)  # Bandwidth adjustment for smoothness\n\n# Labeling and titles\nplt.xlabel(\"Normalized Time Within Response\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimation of Sentiment Progression\")\n\n# Customize the legend\nplt.legend(title=\"Sentiment\", labels=[\"Positive\", \"Negative\"])\n\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:33:01.491174Z","iopub.execute_input":"2025-02-25T20:33:01.491529Z","iopub.status.idle":"2025-02-25T20:33:01.799665Z","shell.execute_reply.started":"2025-02-25T20:33:01.491503Z","shell.execute_reply":"2025-02-25T20:33:01.798788Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summarization\n\nThat is cool. But one of the things we might want to do is actually pick out some key take homes in natural language form. Kind of like a summary of what people thought, not just the sentiment. The first thing we might want to do is simply plot the embeddings of the responses to see how different they are from each other. For this we'll import the base model of BERT (not fine tuned on any downstream tasks).\n\n","metadata":{}},{"cell_type":"code","source":"# Load data\ndata = df\n\n# Initialize the Hugging Face feature-extraction pipeline\nfeature_extractor = pipeline('feature-extraction', model='bert-base-uncased', tokenizer='bert-base-uncased')\n\n# Function to get embeddings for responses using the pipeline\ndef get_bert_embeddings(texts, batch_size=16):\n    all_embeddings = []\n    \n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n        batch_texts = texts[i:i + batch_size]\n        \n        # Get embeddings for the batch\n        embeddings_batch = feature_extractor(batch_texts.tolist())\n        \n        # Extract the [CLS] token embedding from each response in the batch\n        cls_embeddings = [embedding[0][0] for embedding in embeddings_batch]  # [0][0] gives [CLS] token\n        all_embeddings.extend(cls_embeddings)\n    \n    return all_embeddings\n\n# Get embeddings for all responses\nembeddings = get_bert_embeddings(data['response'])\n\n# Perform PCA on the embeddings\npca = PCA(n_components=2)\npca_result = pca.fit_transform(embeddings)\n\n# Plot the results\npca_df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n\n# Plot the results using Seaborn\nplt.figure(figsize=(10, 8))\nsns.scatterplot(x='PC1', y='PC2', data=pca_df, color='blue', alpha=0.5)\n\n# Add title and labels\nplt.title('PCA of BERT Embeddings', fontsize=16)\nplt.xlabel('Principal Component 1', fontsize=14)\nplt.ylabel('Principal Component 2', fontsize=14)\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:33:07.173846Z","iopub.execute_input":"2025-02-25T20:33:07.174269Z","iopub.status.idle":"2025-02-25T20:33:14.086248Z","shell.execute_reply.started":"2025-02-25T20:33:07.174217Z","shell.execute_reply":"2025-02-25T20:33:14.085354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we sample some disparate examples from this embedding space. We then run a summarization pipeline on these and return the concatenated results. ","metadata":{}},{"cell_type":"code","source":"# Merge the data\n\nmerged_df = pd.concat([data, pca_df], axis=1)\n\n# Calculate the Euclidean distance from the origin (0, 0) using PCA1 and PCA2\nmerged_df['dist_pca'] = np.sqrt(merged_df['PC1']**2 + merged_df['PC2']**2)\n\n# Bin the distances into 5 quantiles using qcut\nmerged_df['quantile'] = pd.qcut(merged_df['dist_pca'], q=5, labels=False)\n\n# Sample one row from each quantile group, excluding the grouping column\nsampled_df = merged_df.groupby('quantile', group_keys=False).apply(lambda x: x.sample(n=1))\n\n# Print the sampled rows (with id, response, PC1, PC2, dist_pca)\nprint(sampled_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:34:48.383571Z","iopub.execute_input":"2025-02-25T20:34:48.383892Z","iopub.status.idle":"2025-02-25T20:34:48.400619Z","shell.execute_reply.started":"2025-02-25T20:34:48.383867Z","shell.execute_reply":"2025-02-25T20:34:48.399775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the summarization pipeline\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\ncombined_responses = \" \".join(sampled_df['response'].tolist())\n\n# Function to generate bullet-point summary\ndef generate_bullet_summary(text):\n    summary = summarizer(text, max_length=200, min_length=100, do_sample=False)[0]['summary_text']\n    bullet_points = \"• \" + summary.replace(\". \", \".\\n• \")\n    return bullet_points\n\n# Generate the bullet-point summary for all responses combined\nbullet_summary = generate_bullet_summary(combined_responses)\n\n# Print the final bullet-point summary\nprint(\"Bullet-point Summary for All Responses:\")\nprint(bullet_summary)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T20:35:43.532854Z","iopub.execute_input":"2025-02-25T20:35:43.533156Z","iopub.status.idle":"2025-02-25T20:35:58.406576Z","shell.execute_reply.started":"2025-02-25T20:35:43.533131Z","shell.execute_reply":"2025-02-25T20:35:58.405664Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Assignments\n\n1. Compare outputs of the last excercise to either an extractive (e.g. BERTSUM) or abstractive summarization model (e.g. like BART, T5) run over the whole dataset (hint: first do summaries of the individual responses, then concatenate and do a summary of the result) Do the take-homes differ from your simple approach of sampling different responses? If so how?\n2. Did you get any truncation of the contexts using the tokenizer? How much were you missing? Update using chunking or additional model (Longformer), how did it change your results?\n3. Generate your own sub queries/questions you want to ask these students, and return the 5 top individual responses that best match your queries using cosine similarity matches and rankings.\n","metadata":{}}]}