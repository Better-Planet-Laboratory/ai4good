{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environmental Rights Violations Chatbot\n",
    "![env-rights](https://www.coe.int/documents/365513/276439533/hrc-healthy-environment-870x489.jpg/e8822dbb-ebb4-5708-d3cf-2356020454db?t=1734513740985)\n",
    "## What is it?\n",
    "Environmental rights means any proclamation of a human right to environmental conditions of a specified quality. Human rights and the environment are intertwined; human rights cannot be enjoyed without a safe, clean and healthy environment; and sustainable environmental governance cannot exist without the establishment of and respect for human rights. This relationship is increasingly recognized, as the right to a healthy environment is enshrined in over 100 constitutions. . Learn more at the United Nations Environment Program (UNEP) page on [Environmental Rights](https://www.unep.org/explore-topics/environmental-rights-and-governance/what-we-do/advancing-environmental-rights/what\n",
    "\n",
    "The Environmental Rights Violations (ERV) Chatbot queries SQL database containing mean, max, min, median, and standard deviation of air quality by country (and by extension, continent) with reasonable but inconsistent accuracy.\n",
    "\n",
    "For example, some user questions that the chatbot may answer are: \n",
    "1. What is the average and standard deviation of air quality in Indonesia?\n",
    "2. Which five countries have the worst air quality?\n",
    "3. What is the average air quality on each continent?\n",
    "4. In how many countries did maximum air quality surpass 100 micrograms per cubic meter?\n",
    "5. What is the average air quality among countries that start with the letter D?\n",
    "\n",
    "## Data sources\n",
    "\n",
    "Data                     | Source                                | Path (`data/input/...`)                     | Description                                                              | Units      | Sp. Res. | Temp. Res.\n",
    "-------------------------|---------------------------------------|---------------------------------------------|--------------------------------------------------------------------------|------------|----------| --------- \n",
    "Global Air Quality       | [WUSTL](https://sites.wustl.edu/acag/datasets/surface-pm2-5-archive/) | `airQuality.tif`                            | Ground-level fine particulate matter (PM2.5)                             | Î¼g/m<sup>3 | ~1 km    | Single mean of years 2014-2019\n",
    "Global Heat Stress       | [CHIRTS](https://data.chc.ucsb.edu/products/CHIRTSdaily/v1.0/global_tifs_p05/)               | `heatstress103.tif`                         | Number of days in year where heat index was greater than 103F            | #          | ~5 km    | Year 2016 (hottest up to 2020)         \n",
    "Global Sanitation Access | [IHME](https://ghdx.healthdata.org/record/ihme-data/lmic-wash-access-geospatial-estimates-2000-2017)                 | `sanitation_access.tif`<sup>[1](#fn1)</sup> | Coverage of any improved sanitation facility access in 90 LMICs          | %          | ~5 km    | Single mean of years 2012-2017<sup>[3](#fn3)</sup>         \n",
    "Global Stunting          | [IHME](https://ghdx.healthdata.org/record/ihme-data/lmic-child-growth-failure-geospatial-estimates-2000-2017)                 | `stunting.tif`<sup>[2](#fn2)</sup>          | Prevalence of stunting for children under 5 years of age for 105 LMICs   | %          | ~5 km    | Single mean of years 2012-2017<sup>[3](#fn3)</sup>          \n",
    "Countries                | [World Bank](https://datacatalog.worldbank.org/search/dataset/0038272/World-Bank-Official-Boundaries)                     | `WB_countries_Admin0_10m`                   | World Bank-approved administrative boundaries (Admin 0), i.e. countries  | NA         | NA       | Last updated Mar 19, 2020        \n",
    "\n",
    "<a name=\"fn1\">1</a>: Previously named `w_access`\n",
    "\n",
    "<a name=\"fn2\">2</a>: Previously named `IHME_LMIC_CGF_2000_2017_STUNTING_PREV_MEAN_2017_Y2020M01D08.tif`\n",
    "\n",
    "## Software\n",
    "\n",
    "This chatbot relies on the LangChain interface and the Llama 3.1 large language model (LLM). We'll use Ollama to download and serve the model in our environment.\n",
    "\n",
    "## Outline\n",
    "1. Setup environment\n",
    "1. Load data into SQL database\n",
    "1. Define unit tests\n",
    "1. Build LLM\n",
    "1. Run LLM on possible queries\n",
    "1. Assignment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "Let's run this notebook with the GPU accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Spatial packages\n",
    "!pip install rasterio==1.4.0 rioxarray==0.17.0 geopandas==1.0.1 shapely==2.0.6 rasterstats==0.20.0 --quiet\n",
    "\n",
    "# LangChain\n",
    "!pip install langchain_ollama==0.2.0 langchain_core==0.3.9 langchain_community==0.3.1 langchain_experimental==0.3.2 langgraph --quiet\n",
    "\n",
    "# Visualization\n",
    "!pip install folium matplotlib mapclassify --quiet\n",
    "print(\"installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:28.437506Z",
     "start_time": "2024-11-15T17:46:24.259369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterstats import zonal_stats\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = '/kaggle/input/erv-chatbot/data'\n",
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data Layers\n",
    "The source data is in [GeoTIFF](https://www.ogc.org/publications/standard/geotiff/) format. Let's visualize a layer interactively. First let's open up the tif file with rasterio and look at some of its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Open the GeoTIFF file\n",
    "tif_filepath = \"/input/airQuality.tif\"\n",
    "src = rio.open(f'{data_path}{tif_filepath}')\n",
    "metadata = src.meta\n",
    "bounds = src.bounds\n",
    "data_range = (src.read(1).min(), src.read(1).max())\n",
    "\n",
    "print(\"Metadata:\", metadata)\n",
    "print(\"Bounds: \", bounds)\n",
    "print(\"Data Range:\", data_range)\n",
    "\n",
    "array = src.read(1)\n",
    "print(\"Shape:\", array.shape)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(array, cmap='inferno')\n",
    "plt.show() \n",
    "\n",
    "# close the image\n",
    "src.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to plot it interactively with Folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# File path\n",
    "tif_filepath = f'{data_path}/input/airQuality.tif'\n",
    "png_filepath = 'airQualityNorm.png'\n",
    "\n",
    "# Open the GeoTIFF file\n",
    "with rio.open(tif_filepath) as src:\n",
    "    data = src.read(1).astype(np.float32)  # Read the first band\n",
    "    nodata_value = src.nodata\n",
    "    bounds = [[src.bounds.bottom, src.bounds.left], [src.bounds.top, src.bounds.right]]\n",
    "\n",
    "# Replace NoData values with NaN\n",
    "data[data == nodata_value] = np.nan\n",
    "\n",
    "# Normalize the data for visualization (0 to 255)\n",
    "vmin, vmax = np.nanmin(data), np.nanmax(data)\n",
    "data = (255 * (data - vmin) / (vmax - vmin)).astype(np.uint8)\n",
    "\n",
    "# Convert array to an image\n",
    "plt.imsave(png_filepath, data, cmap='inferno', format='png')\n",
    "\n",
    "# Create a Folium map\n",
    "m = folium.Map(location=[(bounds[0][0] + bounds[1][0]) / 2, \n",
    "                         (bounds[0][1] + bounds[1][1]) / 2], zoom_start=2)\n",
    "\n",
    "# Add image overlay\n",
    "img_overlay = folium.raster_layers.ImageOverlay(\n",
    "    name=\"Air Quality\",\n",
    "    image=png_filepath,\n",
    "    bounds=[[-50, -180], [57, 180]],\n",
    "    opacity=0.9,\n",
    "    interactive=True\n",
    ")\n",
    "img_overlay.add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Display map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is off, the country boundaries don't quite line up. That's because Folium uses the Mercator projection EPSG:3857 and this image is using ESPG:4326. \n",
    "**Bonus: repoject the tif into Mercator with `rasterio` per [this example](https://rasterio.readthedocs.io/en/stable/topics/reproject.html#reprojecting-a-geotiff-dataset) in the docs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reproject the image into mercator and revisualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into SQL database\n",
    "\n",
    "To interact with this data through a natural language model, we want it in a SQL database. That way, the LLM can turn out questions into SQL queries, and then interpret the returned data to answer our questions. This function defined below takes takes a tif and calculates the per-country statistics, then writes it to a SQL database. Documentation: https://python.langchain.com/docs/how_to/sql_csv/#sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:31.059198Z",
     "start_time": "2024-11-15T17:46:31.054911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate zonal statistics in every polygon\n",
    "def calculate_zonal_stats(list_of_stats: list, geodata_path: Path, raster_path: Path, save_csv_path: Path = None):\n",
    "\n",
    "    # Load data\n",
    "    polygons = gpd.read_file(geodata_path)\n",
    "    with rio.open(raster_path) as src:\n",
    "        if src.crs != polygons.crs:\n",
    "            polygons = polygons.to_crs(src.crs)  # align CRS, if appplicable\n",
    "        raster = src.read(1)\n",
    "        affine = src.transform\n",
    "        nodata_value = src.nodata\n",
    "\n",
    "    # Calculate\n",
    "    stats = zonal_stats(polygons, raster,\n",
    "                        stats=list_of_stats,\n",
    "                        affine=affine, nodata=nodata_value)\n",
    "\n",
    "    # Reattach to original data\n",
    "    polygon_x_raster = pd.concat([\n",
    "        pd.DataFrame(polygons.drop(columns='geometry')),  # drop geometry to reduce size of dataset\n",
    "        pd.DataFrame(stats)],\n",
    "        axis=1)\n",
    "\n",
    "    # Export, if requested\n",
    "    if save_csv_path is not None:\n",
    "        polygon_x_raster.to_csv(save_csv_path, index=False)\n",
    "\n",
    "    return(polygon_x_raster)\n",
    "\n",
    "# Load data to SQL with LangChain\n",
    "def data_to_sql(df, name_of_database: str, if_exists_replace: bool = False):\n",
    "    database_path = f'{data_path}/{name_of_database}.db'\n",
    "    engine = create_engine(f'sqlite:///{database_path}')\n",
    "    if not Path(database_path).exists() or if_exists_replace is True:\n",
    "        df.to_sql(name_of_database, engine, if_exists='replace')\n",
    "    db = SQLDatabase(engine=engine)\n",
    "\n",
    "    return db\n",
    "\n",
    "print('function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:35.174257Z",
     "start_time": "2024-11-15T17:46:32.846440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# calculate zonal stats per country and write to a SQL database\n",
    "stats = ['mean', 'max', 'min', 'median', 'std']\n",
    "df = calculate_zonal_stats(\n",
    "    list_of_stats=stats,\n",
    "    geodata_path=Path(f'{data_path}/input/WB_countries_Admin0_10m/'),\n",
    "    raster_path=Path(f'{data_path}/input/airQuality.tif')\n",
    ")\n",
    "df = df[['WB_NAME', 'CONTINENT'] + stats].dropna()\n",
    "db = data_to_sql(df = df, name_of_database = 'airxcntry', if_exists_replace = False)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:46.894357Z",
     "start_time": "2024-11-15T17:46:46.892551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Potential queries\n",
    "q_simple = \"What is the average and standard deviation of air quality in Indonesia?\"\n",
    "q_relative = \"Which five countries have the worst air quality?\"\n",
    "q_summarize = \"What is the average air quality on each continent?\"\n",
    "q_filter = \"In how many countries did maximum air quality surpass 100 micrograms per cubic meter?\"\n",
    "\n",
    "print(q_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:49.124245Z",
     "start_time": "2024-11-15T17:46:49.110566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Manually find answers from dataframe\n",
    "answer_key = {}\n",
    "\n",
    "# 1) \"What is the average and standard deviation of air quality in Indonesia?\"\n",
    "answer_key = answer_key | {\n",
    "    'q_simple': df[df['WB_NAME'] == \"Indonesia\"][['mean', 'std']].round(2).to_dict('records')\n",
    "}\n",
    "\n",
    "# 2) \"Which five countries have the worst air quality?\"\n",
    "answer_key = answer_key | {\n",
    "    'q_relative': df.sort_values('mean', ascending=False).head(5)['WB_NAME'].values.tolist()\n",
    "}\n",
    "\n",
    "# 3) \"What is the average air quality on each continent?\"\n",
    "answer_key = answer_key | {\n",
    "    'q_summarize': df.groupby('CONTINENT')['mean'].mean().round(2).to_dict()\n",
    "}\n",
    "\n",
    "# 4) \"In how many countries did maximum air quality surpass 100 micrograms per cubic meter?\"\n",
    "answer_key = answer_key | {\n",
    "    'q_filter': df[df['max'] > 100]['WB_NAME'].count()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:50.443719Z",
     "start_time": "2024-11-15T17:46:50.439126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "answer_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LLM\n",
    "\n",
    "We will eventually chain together agents that a) turn question into SQL query, b) run that SQL query, and c) return SQL result using a natural language response; but first let's explore each agent in turn.\n",
    "\n",
    "Resources:\n",
    "* Documentation: https://python.langchain.com/docs/tutorials/sql_qa\n",
    "* Example Notebook for Running Ollama in Kaggle: https://www.kaggle.com/code/sumanmichael/ollama-langchain-in-kaggle-gpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# download and install ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# to support background processes in Kaggle\n",
    "import os\n",
    "get_ipython().system = os.system\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# start ollama in the background\n",
    "!ollama serve &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# use ollama to pull the Llama v3.1\n",
    "!ollama pull llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:54.319690Z",
     "start_time": "2024-11-15T17:46:53.147545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:46:54.957736Z",
     "start_time": "2024-11-15T17:46:54.935200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = ChatOllama(model=\"llama3.1\", seed=123)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Generate SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:47:07.419996Z",
     "start_time": "2024-11-15T17:46:58.270893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# a) Generate SQL query (default)\n",
    "sql_agent_default = create_sql_query_chain(model, db)\n",
    "sql_code_default = sql_agent_default.invoke({\"question\": q_simple})\n",
    "print(sql_code_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question for you: Is this SQL query going to run successfully? Why or why not?**\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:47:51.223169Z",
     "start_time": "2024-11-15T17:47:50.413001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "db.run(sql_code_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! It hit an error. Something is wrong with our query. Let's see if we can do some prompt engineering to improve model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:04.079473Z",
     "start_time": "2024-11-15T17:48:04.045196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Default prompt\n",
    "sql_prompt_default = sql_agent_default.get_prompts()[0]\n",
    "sql_prompt_default.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompt engineering\n",
    "\n",
    "Remove the following instructions, which may be confusing the model: \n",
    "* Pay attention to use date('now') function to get the current date, if the question involves \"today\".\n",
    "* Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
    "\n",
    "And add the following instructions based on how we see the model interpreting our request. \n",
    "* Return ONLY the SQLQuery with no other context given. Do not include the question in the output. Do not include the title 'SQLQuery' in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:06.599896Z",
     "start_time": "2024-11-15T17:48:06.597206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Improved prompt\n",
    "sql_prompt_text = '''\n",
    "    You are a SQLite expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\n",
    "    Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\n",
    "    Never query for all columns from a table. You must query only the columns that are needed to answer the question. \n",
    "    Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "    Return ONLY the SQLQuery with no other context given. Do not include the question in the output. Do not include the title 'SQLQuery' in the output.\n",
    "    \n",
    "    Use the following format:\n",
    "    Question: Question here\n",
    "    SQLQuery: SQL Query to run\n",
    "    SQLResult: Result of the SQLQuery\n",
    "    Answer: Final answer here\n",
    "    \n",
    "    Only use the following tables:\n",
    "    {table_info}\n",
    "    \n",
    "    Question: {input}\n",
    "'''\n",
    "\n",
    "sql_prompt = PromptTemplate(\n",
    "    input_variables = [\"input\", \"table_info\", \"top_k\", \"dialect\"],\n",
    "    template=sql_prompt_text\n",
    ")\n",
    "\n",
    "sql_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:07.411414Z",
     "start_time": "2024-11-15T17:48:07.408186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# New agent based on improved prompt\n",
    "sql_agent = create_sql_query_chain(model, db, prompt=sql_prompt)\n",
    "sql_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what our new model returns when asked to generate SQL code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:12.083594Z",
     "start_time": "2024-11-15T17:48:09.084468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sql_code = sql_agent.invoke({\"question\": q_simple})\n",
    "print(sql_code)\n",
    "print(db.run(sql_code))\n",
    "\n",
    "print(\"\\nAnswer key:\")\n",
    "answer_key['q_simple']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We haven't actually built another agent to execute the SQL query yet, so let's do that now; and chain it to the agent that writes SQL code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Execute SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:18.776196Z",
     "start_time": "2024-11-15T17:48:18.003412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# b) Execute SQL query \n",
    "execute_sql_query = QuerySQLDataBaseTool(db=db)\n",
    "write_sql_query = create_sql_query_chain(model, db, prompt=sql_prompt)\n",
    "sql_chain = write_sql_query | execute_sql_query\n",
    "sql_chain.invoke({\"question\": q_simple})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Natural language response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:20.173707Z",
     "start_time": "2024-11-15T17:48:20.171490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prompt for agent that will interact with user\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Result: {result}\n",
    "    Answer:\n",
    "    \n",
    "    \"\"\"\n",
    ")\n",
    "answer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:22.706591Z",
     "start_time": "2024-11-15T17:48:22.700630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Chain all agents together\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_sql_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_sql_query\n",
    "    )\n",
    "    | answer_prompt | model | StrOutputParser()\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:26.378552Z",
     "start_time": "2024-11-15T17:48:23.323118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test chain\n",
    "final_response = chain.invoke({\"question\": q_simple})\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to other queries\n",
    "Let's test out our Chatbot now on the test questions we set up before. After each one, we'll evaluate how it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:48:44.355489Z",
     "start_time": "2024-11-15T17:48:33.333587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# q_relative \n",
    "# q_relative = \"Which five countries have the worst average air quality?\"\n",
    "\n",
    "print(q_relative, \"\\n\")\n",
    "\n",
    "r_relative = chain.invoke({\"question\": q_relative})\n",
    "print(r_relative)\n",
    "\n",
    "print(\"\\nAnswer key:\", answer_key['q_relative'])\n",
    "\n",
    "print(\"\\nSQL code written:\", sql_agent.invoke({\"question\": q_relative}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How did the Chatbot do at answering this question? What would you suggest could be done to improve the answer?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:49:14.042082Z",
     "start_time": "2024-11-15T17:49:04.589220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# q_summarize\n",
    "\n",
    "print(q_summarize, \"\\n\")\n",
    "\n",
    "r_summarize = chain.invoke({\"question\": q_summarize})\n",
    "print(r_summarize)\n",
    "\n",
    "print(\"\\nAnswer key:\", answer_key['q_summarize'])\n",
    "\n",
    "print(\"\\nSQL code written:\", sql_agent.invoke({\"question\": q_summarize}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How did the Chatbot do at answering this question? Was the answer correct? If not, where did the answer come from? What would you suggest could be done to improve the answer?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T17:49:34.931943Z",
     "start_time": "2024-11-15T17:49:22.735309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# q_filter\n",
    "\n",
    "print(q_filter, \"\\n\")\n",
    "\n",
    "r_filter = chain.invoke({\"question\": q_filter})\n",
    "print(r_filter)\n",
    "\n",
    "print(\"\\nAnswer key:\", answer_key['q_filter'])\n",
    "\n",
    "print(\"\\nSQL code written:\", sql_agent.invoke({\"question\": q_filter}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How did the Chatbot do at answering this question? Was the answer correct? Did the SQL query seem appropriate? What would you suggest could be done to improve the answer?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "**Part 1 (easier):** Spend some time tweaking the prompts for the ERV Chatbot above to see if you can improve the quality of responses to one of the test questions where the Chatbot did poorly. What did you change? Did it help?\n",
    "\n",
    "**Part 2 (harder):** Pick another one of the input data layers (the one you were assigned) and repeat the steps in this notebook to make it queryable by the ERV Chatbot (ie visualize the data, compute the zonal stats, convert to a SQL database, engineer the prompt, and set up 2-3 test questions). Write the code below.\n",
    "\n",
    "## Bonus Assignment\n",
    "**Part 1 (easy):** What could we do to improve this notebook?\n",
    "\n",
    "**Part 2 (medium):** Here we used Llama 3.1 from Meta. Check out other LLMs in the Ollama library, find one that you think might perform better, and pull it into this notebook to see how it does.\n",
    "\n",
    "**Part 3 (very hard):** Adapt the ERV Chatbot to be able to answer queries about multiple types of Environmental Rights Violations (ie air quality AND sanitation access).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "Thank you to [Zia Mehrabi](https://www.colorado.edu/envs/zia-mehrabi) for ideating a chatbot that can help people discover human rights violations. Thank you to [Naia Ormaza Zulueta](https://www.colorado.edu/envs/naia-ormaza-zulueta) for compiling the datasets and thank you to [Katie Fankhauser](https://www.linkedin.com/in/katie-fankhauser/) for writing most of the code. [Isaiah Lyons-Galante](https://www.colorado.edu/envs/isaiah-lyons-galante) adapted the notebook for Kaggle and for the AI for Good course. All contributions were made as members of the Better Planet Lab."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6781963,
     "sourceId": 10910232,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
