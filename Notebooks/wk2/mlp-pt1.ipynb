{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"_is_fork":false,"_change_revision":0,"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to Multi-Layer Perceptrons\nThis notebook will take you through the basics of creating multi-layer perceptrons, one of the oldest models in machine learning and still one of the best universal function approximators in computer science. Below is an illustration of an extremely simplified MLP that takes in 3 inputs, has one hidden layer with 4 nodes, and outputs a single prediction. We'll build a very similar MLP to classify irises into their species based on the petal and sepal qualities. This notebook borrows heavily from [this Iris ML notebook](https://www.kaggle.com/code/ash316/ml-from-scratch-with-iris/data) and [this one as well](https://www.kaggle.com/code/mohitchaitanya/simple-iris-dataset-classification-using-pytorch).\n\nLearning outcomes:\n1. Pull in data on Kaggle\n2. Inspect and explore the data\n3. Split the dataset into train/test\n4. Train classical machine learning models on the data\n5. Build a simple ANN with Pytorch for classification\n\n\n    <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\" alt=\"Iris versicolor\" style=\"width:40%;\">\n    <img src=\"https://isaiahlg.com/portfolio/csci5922/mod2/nn1.png\" alt=\"Basic MLP\" style=\"width:37%;\">","metadata":{"_uuid":"8807ae46aec34ec427b81344d64a3c0d607b258a","_cell_guid":"b7dbedec-9483-4b22-484d-b8a8e04c49ae"}},{"cell_type":"markdown","source":"## Introducing the Iris Dataset\n\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set): \"The Iris flower data set or Fisher's Iris data set is a multivariate data set used and made famous by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gasp√© Peninsula 'all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus.'\n\nThe data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish each species. Fisher's paper was published in the Annals of Eugenics (today the Annals of Human Genetics).\"","metadata":{}},{"cell_type":"markdown","source":"## Importing Data into Kaggle Notebooks\nWe want our iris data into this notebook. To do this:\n- Go to the \"Input\" section on the right\n- Click \"Add Input\"\n- Click \"Datasets\" to search for just datasets\n- Search \"iris\"\n- Import the first result called \"Iris Species\" by clicking the +\nThe dataset should appear in the right-hand menu under datasets.\n\nNote: this notebook can be run with the standard CPU since the dataset is quite simple. We also shouldn't need \"internet on\" for this one.","metadata":{}},{"cell_type":"markdown","source":"## Import Necessary Libraries\nRun this code to import our data manipulation and visualization libaries. You should see the Iris dataset printed out from the directory \"/kaggle/input\".","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt # plotting","metadata":{"_uuid":"43047631b7881a23c63a655f5214b2ebaff946fa","_cell_guid":"c8b7047a-c84c-c0ee-054d-a0c30609cc43","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure the Current Working Directory\nBy default, our notebook is looking in `/kaggle/working/` for files. But the dataset we just imported is in `/kaggle/input/`. Let's set the working directory to just `/kaggle/` so we can use relative file paths for the rest of the notebook.","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(\"The default working directory is:\", os.getcwd())\n\n# Let's set the working directory to just kaggle\nos.chdir('/kaggle') # modify here if not using kaggle to your actual current directory\nprint(\"Now the working directory is:\", os.getcwd())\n\n# Now let's look at the input subdirectory of our working directory for our iris dataset\nfor dirname, _, filenames in os.walk('input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import and Inspect the Dataset","metadata":{}},{"cell_type":"code","source":"iris = pd.read_csv(\"input/iris/Iris.csv\") #load the dataset with pandas\niris.head(2) #show the first 2 rows from the dataset","metadata":{"_uuid":"db8aed63638fcf4833c050541cc0fe5a0b756670","_cell_guid":"ced2723b-e83e-6aa0-4ffb-9ace2cc4a5e3","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"iris.info()  #checking if there is any inconsistency in the dataset\n#as we see there are no null values in the dataset, so the data can be processed","metadata":{"_uuid":"46f49d63be72b4eec75da299e355d6dbaa19f65b","_cell_guid":"4f9370f1-0672-0d8c-4f21-c7eeb694042c","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop the \"Id\" column that we won't use\niris.drop('Id',axis=1,inplace=True) #dropping the Id column as it is unecessary, axis=1 specifies that it should be column wise, inplace =1 means the changes should be reflected into the dataframe","metadata":{"_uuid":"c9b2172cc499bda6d56d2199b8d286217abbd677","_cell_guid":"af6dd1be-5c24-27f3-6319-eb6af2c65c27","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploratory Data Analysis with the Dataset\nLet's start by plotting some of the input variables and coloring them by the label (species). This helps give us a sense of what the input data look like, and whether this is going to be an easy classification task.","metadata":{"_uuid":"f00690de3343002575e680a83ae395ef0060fe1f","_cell_guid":"a2e19920-d24b-7551-10c4-e1088c8a4324"}},{"cell_type":"code","source":"fig = iris[iris.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='setosa')\niris[iris.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length versus Sepal Width by Iris Species\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","metadata":{"_uuid":"c9fd2c2b0eb51ada3a4f57eb51023cb080a0d308","_cell_guid":"09a16bf0-067b-8da0-3eed-2014dc8cfec7","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above graph shows relationship between the sepal length and width. Now we will check relationship between the petal length and width.","metadata":{"_uuid":"d0793e117fd202f679b6ecb1239e0ab0a8cfd137","_cell_guid":"e853b9fa-a1db-cdc8-1f8f-0f5f2f2d1ab6"}},{"cell_type":"code","source":"fig = iris[iris.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='setosa')\niris[iris.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\niris[iris.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length versus Petal Width by Iris Species\")\nfig=plt.gcf()\nfig.set_size_inches(10,6)\nplt.show()","metadata":{"_uuid":"16a42e3a6615e48f7b8ed0a6dbb380de74dcd2c8","_cell_guid":"ea5060b8-4067-cf46-99d6-a27be10a7e18","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we can see that the petal features are giving a better cluster division compared to the sepal features. This is an indication that the petals can help in better and accurate predictions over the sepal. We will check that later.","metadata":{"_uuid":"dd90c7e433f42fea8e5836c91959429710491347","_cell_guid":"3068e91a-2455-f7ae-b1d0-a2114b78ea62"}},{"cell_type":"markdown","source":"### Now let us see how are the length and width are distributed","metadata":{"_uuid":"aaa439a16175b442a42e845060b7fe1bd2d246f0","_cell_guid":"ac6bb577-9975-39d5-aa20-376e574e703c"}},{"cell_type":"code","source":"iris.hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","metadata":{"_uuid":"0ca23638e6e067ffd7501552f346674646e3e1d2","_cell_guid":"d818068d-5110-c64e-ec6e-92bda44a9723","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Now let us see how the length and width vary according to the species","metadata":{"_uuid":"3e8d0f657f996be6f4020b1850148bade9fdc588","_cell_guid":"cdb55848-d7eb-d66c-dec1-79cbbfba2826"}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='Species',y='PetalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='Species',y='SepalWidthCm',data=iris)","metadata":{"_uuid":"15e65581708a5b67ac8f0c1994fa08ef8158e799","_cell_guid":"a1d57f07-5c6b-4ab3-ad15-b8b5245c05b9","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The violinplot shows density of the length and width in the species. The thinner part denotes that there is less density whereas the fatter part conveys higher density","metadata":{"_uuid":"0e907abebd6751fa3597fd6b08bb24eb13081443","_cell_guid":"2497705a-9c08-5d8b-6ca0-0edc509e73b9"}},{"cell_type":"markdown","source":"## Setting Up Classical Machine Learning\n\n**Classification**: samples belong to two or more classes and we want to learn from already labeled data how to predict the class of unlabeled data\n\n**Regression**: if the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight.\n\nBefore we start, let's define some ML terms:\n\n**Attribute**-->An attribute is a property of an instance that may be used to determine its classification. In the following dataset, the attributes are the petal and sepal length and width. It is also known as a **feature**.\n\n**Target variable**, in the machine learning context is the variable that is or should be the output. Here, the target variable is the flower species.","metadata":{"_uuid":"b45c7ca882bf32568268465f041282cfcd0e6fe9","_cell_guid":"4df5f118-994c-4d7d-2fa1-b8a1ed1a82ec"}},{"cell_type":"code","source":"pip install","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# importing alll the necessary packages to use the various classification algorithms\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn import metrics #for checking the model accuracy\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm","metadata":{"_uuid":"2b652b2398f95bacb6286eac49694cd422a54e6a","_cell_guid":"c27e7e16-6083-5b53-cda4-c43cf4c79a67","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"iris.shape #get the shape of the dataset","metadata":{"_uuid":"15457e7fa8ff19b86c2858c365f11fa289fca723","_cell_guid":"d967de9a-df34-bc84-899b-28b8804f7d58","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Steps To Be followed When Applying an Algorithm\n\n 1. Split the dataset into training and testing dataset. The testing dataset should be smaller than training one as it will help in training the model better. A good rule of thumb is 80/20 train/test, or 60/20/20 if you're have train/validate/test. 80/10/10 can be better for large datasets.\n 2. Select an appropriate model that fits your task (ie classification, regression, etc)..\n 3. Then pass the training dataset to the algorithm to train it. We use the **.fit()** method\n 4. Then pass the testing data to the trained algorithm to predict the outcome. We use the **.predict()** method.\n 5. We then check the accuracy by **passing the predicted outcome and the actual output** to the model.","metadata":{"_uuid":"362675378d1f8d7892597877aa9dbd1caa20c0ff","_cell_guid":"74afdd84-2b1f-b6e3-b9e2-6c076e44cb10"}},{"cell_type":"markdown","source":"### Splitting The Data into Training And Testing Dataset","metadata":{"_uuid":"ffa8b5a66eb4651f46672b7263774dbcf125ecbf","_cell_guid":"2de179a6-ea22-00ff-8756-63d3aef6bd2d"}},{"cell_type":"code","source":"train, test = train_test_split(iris, test_size = 0.3)# in this our main data is split into train and test\n# the attribute test_size=0.2 splits the data into 80% and 20% ratio. train=80% and test=20%\nprint(\"Training data shape:\", train.shape)\nprint(\"Testing data shape:\", test.shape)","metadata":{"_uuid":"87bc90a957a8defb5e24b886b641fb0a9aa5fc42","_cell_guid":"a24c3ab9-8c7d-2a78-113f-d337c5c61f09","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# let's set the features into a variable X and the label into variable y\ntrain_X = train[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]# taking the training data features\ntrain_y=train.Species# output of our training data\ntest_X= test[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] # taking test data features\ntest_y =test.Species   #output value of test data","metadata":{"_uuid":"7a64738b8be79ad6b0f4899e985d3ab5e475f9e1","_cell_guid":"54dd7ded-5079-fc8c-065b-70bfbfb0b83a","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's peek at the train input features and label...","metadata":{"_uuid":"0b18f7458fd76e93ed4a3932001f6f657cadb647","_cell_guid":"c7d21e9f-8470-9aae-51ef-feea2cec6b1c"}},{"cell_type":"code","source":"train_X.head()","metadata":{"_uuid":"8fa3e8f9f4ba0b27796008dd998c41670f1ffd95","_cell_guid":"2cf23cff-74d0-b15e-87a9-5ed72f543380","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_y.head()  ##output of the training data","metadata":{"_uuid":"6e5039b56dec20594cdd457127d39c0009f5572a","_cell_guid":"60f7b821-4335-e2e5-b868-3d6f850ea927","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Support Vector Machine (SVM)\nA Support Vector Machine (SVM) is a powerful machine learning algorithm widely used for both linear and nonlinear classification, as well as regression and outlier detection tasks. SVMs are highly adaptable, making them suitable for various applications such as text classification, image classification, spam detection, handwriting identification, gene expression analysis, face detection, and anomaly detection.\n\nSVMs are particularly effective because they focus on finding the maximum separating hyperplane between the different classes in the target feature, making them robust for both binary and multiclass classification. In this outline, we will explore the Support Vector Machine (SVM) algorithm, its applications, and how it effectively handles both linear and nonlinear classification, as well as regression and outlier detection tasks.\n\nOne place to learn more is [here](https://www.geeksforgeeks.org/support-vector-machine-algorithm/), but there are so many places to learn!","metadata":{"_uuid":"41536ca3ef126cf761c2b8aca8f02ffa407a5996","_cell_guid":"82010322-d6f3-467d-f1da-4cb3fc82d0bd"}},{"cell_type":"code","source":"model = svm.SVC() #select the algorithm\nmodel.fit(train_X,train_y) # we train the algorithm with the training data and the training output\nprediction=model.predict(test_X) #now we pass the testing data to the trained algorithm\nprint('The accuracy of the SVM is:',metrics.accuracy_score(prediction,test_y))#now we check the accuracy of the algorithm. ","metadata":{"_uuid":"94ed6a0a8a1acc6c42e37db3e50ab6f9380e17f8","_cell_guid":"be869394-0f6d-f062-dd1f-8d2c68f06104","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The SVM does pretty well! Let's check some other models.","metadata":{"_uuid":"9e9fb51ffb0823fb9a7459ad0f164e52a800c15a","_cell_guid":"78e50a4b-ea63-c546-a8c4-07f4e4084c27"}},{"cell_type":"markdown","source":"### Logistic Regression\nLogistic regression is a supervised machine learning algorithm used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not. Logistic regression is a statistical algorithm which analyze the relationship between two data factors. Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1. For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 otherwise it belongs to Class 0. It‚Äôs referred to as regression because it is the extension of linear regression but is mainly used for classification problems.\n\nLearn more about logistic regression [here](https://www.geeksforgeeks.org/understanding-logistic-regression/).","metadata":{"_uuid":"a2d76e51ba5e61a1a6f19675295f4405c2633eba","_cell_guid":"35d98035-f0ae-7d10-fe90-ec13865c5a14"}},{"cell_type":"code","source":"model = LogisticRegression(max_iter = 1000)\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,test_y))","metadata":{"_uuid":"2b53cac02465732a986be2463b3f3411ab043716","_cell_guid":"ca772378-d3c4-4c87-ada6-b9d50a383b01","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Decision Tree\nA decision tree is a flowchart-like structure used to make decisions or predictions. It consists of nodes representing decisions or tests on attributes, branches representing the outcome of these decisions, and leaf nodes representing final outcomes or predictions. Each internal node corresponds to a test on an attribute, each branch corresponds to the result of the test, and each leaf node corresponds to a class label or a continuous value.\n\nLearn more about decision trees [here](https://www.geeksforgeeks.org/decision-tree/). ","metadata":{"_uuid":"756183a2d33bfd269b75b11ce7980f7f22925004","_cell_guid":"7dfab2de-ebb7-4864-675c-aafef45db7a1"}},{"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction,test_y))","metadata":{"_uuid":"3c8158a18da7910b9ba8c3a5169a3009fb27c84f","_cell_guid":"f918247c-a76b-4c58-5145-1fb0e8ab70b4","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### K-Nearest Neighbors\nThe K-Nearest Neighbors (KNN) algorithm is a supervised machine learning method employed to tackle classification and regression problems. Evelyn Fix and Joseph Hodges developed this algorithm in 1951, which was subsequently expanded by Thomas Cover. The article explores the fundamentals, workings, and implementation of the KNN algorithm.\n\nLearn more about how K-Nearest Neighbors works [here](https://www.geeksforgeeks.org/k-nearest-neighbours/).","metadata":{"_uuid":"3410b37d9410b8d9f0ebd5c6509b3250ed7714c1","_cell_guid":"3e4594d9-5fe6-0568-1fc6-e02975278305"}},{"cell_type":"code","source":"model=KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction,test_y))","metadata":{"_uuid":"dda9e73ae95627994e68db9542d769b897d43bec","_cell_guid":"bcd55de7-cfab-c81d-25a8-1db8f0deee94","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Findings\nIt seems that these classical machine learning models are pretty good at distinguishing between the various flower types based on the sepal and pedal attributes. Perhaps this task is a little bit too easy. As a follow up task, we'll try using less information (either sepal or pedal dimensions), and see how well the models do. After that, we'll move onto a more challenging dataset.","metadata":{}},{"cell_type":"markdown","source":"## Task\n\nRepeat the above analysis but using either just the pedal dimensions or the sepal dimensions as input features. ","metadata":{}},{"cell_type":"code","source":"## get coding here (though you'll likely need more cells.)","metadata":{"_uuid":"41c25d25019131c90859443d37b93d8fbb0fb33f","_cell_guid":"14bcdd2b-fc7c-42e2-88c7-ed5d221ecbef","_execution_state":"idle","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deliverable\nWrite a short paragraph that addresses these questions. Which input features do you expect to perform better, sepal or pedal dimensions? Which one actually performs better? Why do you think that is? Is there a way you could have known this without running it? How does it compare to using all of the features as predictors? What do you think this machine learning task is relatively easy?\n\n\n**Your response:**\n\n\nNow download and submit your notebook on Discord!\n","metadata":{}}]}