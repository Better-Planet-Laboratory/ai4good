{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8375157,"sourceType":"datasetVersion","datasetId":4979622}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction to MLPs using US Census Income Data\nThis week, we'll use a simple MLP to predict whether an individual makes over or under $50,000 per year based on some demographic characteristics. This notebook was created largely from [this one](https://www.kaggle.com/code/dogukantabak/income-prediction-pytorch). The dataset we'll use is on Kaggle [here](https://www.kaggle.com/datasets/jainaru/adult-income-census-dataset/data).  \n\nLearning outcomes:\n1. Pull in data on Kaggle\n2. Inspect and explore the data\n3. Split the dataset into train/test\n4. Train classical machine learning models on the data\n5. Build and train simple ANN with Pytorch for classification\n\n<img src=\"https://isaiahlg.com/portfolio/csci5922/mod2/nn1.png\" alt=\"Basic MLP\" style=\"width:37%;\">","metadata":{}},{"cell_type":"markdown","source":"## Income Predictor Dataset - US Adult\nLink: https://www.kaggle.com/datasets/jainaru/adult-income-census-dataset/data\n\nThe Adult Census Income dataset, extracted from the 1994 US Census Database by Barry Becker, serves as a valuable resource for understanding the intricate interplay between socio-economic factors and income levels. Comprising anonymized information such as occupation, age, native country, race, capital gain, capital loss, education, work class, and more, this dataset offers a comprehensive view of the American demographic landscape.\n\n**Dataset Overview**\nThe dataset consists of two CSV files: adult-training.txt and adult-test.txt, each row representing an individual. Key features include occupation, age, native country, race, capital gain, capital loss, education, work class, and more. The target variable, 'income_bracket', categorizes individuals into two groups: \">50K\" and \"<=50K\".\n\n**Exploration and Preprocessing**\nExploring the dataset reveals a mix of categorical and continuous features, as well as missing values. Understanding the distribution and relationships of these features is crucial for feature selection and data preprocessing, including handling missing values and encoding categorical variables.\n\n**Modeling and Evaluation**\nTo predict income levels, various classifiers can be trained on the training dataset and evaluated using the test dataset. Algorithms such as logistic regression, decision trees, random forests, and neural networks can be employed based on the dataset's complexity and the desired performance metrics.","metadata":{}},{"cell_type":"markdown","source":"## Importing Data into Kaggle Notebooks\nWe want our income data into this notebook. To do this:\n- Go to the \"Input\" section on the right\n- Click \"Add Input\"\n- Click \"Datasets\" to search for just datasets\n- Search \"Income Predictor\"\n- Import the first result called \"Income Predictor Dataset- US Adult\" by clicking the +\nThe dataset should appear in the right-hand menu under datasets.\n\nNote: this notebook can be run with the standard CPU since the dataset is quite simple. We also shouldn't need \"internet on\" for this one.","metadata":{}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Exploration & Cleaning","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/adult-income-census-dataset/adult.csv') # read in the file\ndata.head() # look at the first few rows of the dataframe","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.info() # look at the various columns and their data types","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# look for null values and duplicate rows\nprint(\"Null values by column:\\n\", data.isnull().sum())\nprint(\"Number of duplicate rows: \", data.duplicated().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clean the data and reinspect\ndata.drop_duplicates(inplace=True) # drop the duplicate rows\ndata.replace('?', np.nan, inplace=True) # replace any values with a ? with \"NaN\" or \"Not a Number\"\ndata.dropna(inplace=True) # drop any rows that have NA values\ndata.info() # inspect data again","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that we've dropped all duplicate rows and all rows with ? or null values, next we need to convert our variables with categorical answers to numeric values, extract our label, and scale our numeric values. For this, we'll use the classical machine learning library `sci-kit-learn`. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# one hot encode the categorical features\ncategorical_features = ['workclass', 'education', 'marital.status', 'occupation', \n                        'relationship', 'race', 'sex', 'native.country', 'income']\n\nfor feature in categorical_features:\n    le = LabelEncoder()\n    data[feature] = le.fit_transform(data[feature])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract the label column\nX = data.drop('income', axis=1)\ny = data['income']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# scale the numeric features to each have a mean of 0, std dev of 1\ncontinuous_features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\nscaler = StandardScaler()\nX[continuous_features] = scaler.fit_transform(X[continuous_features])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the data into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PyTorch & Tensors\nTensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n\nTensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API.\n\nLearn more here: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n\nTo start let's import the libraries we need from PyTorch.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# convert test and training data to a tensor\nX_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test.values, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Datasets & Dataloaders\nCode for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n\nLearn more here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html","metadata":{}},{"cell_type":"code","source":"# create a TensorDataset within Pytorch\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# wrap the Dataset in a DataLoader to be iterable\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Get Device for Training\nWe want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let’s check to see if torch.cuda or torch.backends.mps are available, otherwise we use the CPU.","metadata":{}},{"cell_type":"code","source":"device = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build a Machine Learning Model\n\nHere's the fun part, building the machine learning model! Pytorch makes this relatively straightforward. Let's start with a very simple model.","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 2),\n            nn.ReLU(),\n        )\n\n    def forward(self, x):\n        x = self.linear_relu_stack(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# figure out the width of the input tensor\nprint('The shape of the X_train_tensor is:', X_train_tensor.shape)\n# let's use the second value, the # of columns\ninput_dim = X_train_tensor.shape[1]\n\n# instantiate the model\nmodel = NeuralNetwork(input_dim)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train a Neural Network\n\nFirst step is to define our training parameters. The three key ones are:\n1. Loss Function (https://pytorch.org/docs/stable/nn.html#loss-functions)\n2. Learning Rate (https://www.geeksforgeeks.org/impact-of-learning-rate-on-a-model/)\n3. Optimizer (https://pytorch.org/docs/stable/optim.html) ","metadata":{}},{"cell_type":"code","source":"# define a loss function\ncriterion = nn.CrossEntropyLoss() # a go to for classication problems\nlearning_rate = 0.001 # a standard starting point, use factors of 10\noptimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam Optimizer: https://arxiv.org/abs/1412.6980","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now it's time for the training loop. The following code will define how many times we want to loop over the training data, and then executes that loop, running the data through the model with each batch, calculating the loss, and updating the model parameters accordingly.","metadata":{}},{"cell_type":"code","source":"# Set the number of times to iterate over the entire training dataset\nnum_epochs = 10\n\n# Start the training loop, iterating through the dataset `num_epochs` times\nfor epoch in range(num_epochs):  # Loop over each epoch\n    model.train()  # Put the model into training mode (enables features like dropout)\n    running_loss = 0.0  # Initialize a variable to keep track of cumulative loss for the epoch\n\n    # Loop through each batch of data in the training dataset\n    for inputs, labels in train_loader:  # `inputs` are the features, `labels` are the targets\n        optimizer.zero_grad()  # Clear the gradients from the previous step\n        \n        outputs = model(inputs)  # Perform a forward pass through the model to get predictions\n        loss = criterion(outputs, labels)  # Compute the loss between predictions and actual labels\n        loss.backward()  # Perform backpropagation to calculate gradients of loss with respect to parameters\n        optimizer.step()  # Update model parameters based on the gradients\n        \n        running_loss += loss.item()  # Accumulate the loss for this batch\n    \n    # Calculate the average loss for this epoch\n    avg_loss = running_loss / len(train_loader)  # Divide total loss by the number of batches\n    # Print progress, showing the current epoch and average loss for the epoch\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation\nThe code below runs the test data through our trained model, and reports on the performance. Remember, the model did not see this data in training.","metadata":{}},{"cell_type":"code","source":"model.eval()  # Put the model in evaluation mode (disables features like dropout and gradient tracking)\ncorrect = 0  # Initialize a counter for correctly classified samples\ntotal = 0  # Initialize a counter for the total number of samples\n\nwith torch.no_grad():  # Disable gradient calculation for efficiency and to save memory\n    for inputs, labels in test_loader:  # Loop through each batch in the test dataset\n        outputs = model(inputs)  # Perform a forward pass through the model to get predictions\n        _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability for each sample\n        total += labels.size(0)  # Update the total count with the number of samples in this batch\n        correct += (predicted == labels).sum().item()  # Increment the correct count for accurate predictions\n\naccuracy = 100 * correct / total  # Calculate accuracy as a percentage\nprint(f'Accuracy on test data: {accuracy:.2f}%')  # Display the accuracy of the model on the test data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving & Loading our Model\nA common way to save a model is to serialize the internal state dictionary (containing the model parameters).","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The process for loading a model includes re-creating the model structure and loading the state dictionary into it\nmodel = NeuralNetwork(input_dim)\nmodel.load_state_dict(torch.load(\"model.pth\", weights_only=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This model can now be used to make predictions.\nclasses = [\"Over $50k\", \"Under $50k\"] # is this correct, or should it be switched?\nrow_index_to_test = 6\n\n# evaluate the model on this one item from the dataset\nmodel.eval()\nx, y = test_dataset[row_index_to_test][0], test_dataset[row_index_to_test][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Assignment\n\n1. Play with the model. Consider adding more layers, changing the size of layers, adding things like drop out, etc. Full list of types is here: torch.nn\n2. Play with the optimization parameters. Try other learning rates, more or less epochs, different loss functions, optimizers, etc. See which one gets a good result fastest.\n3. Reflect on the question, what are some ethical considerations for building a model that classifies people as high or low earners based on their demographics?\n\n## Bonus Challenge #1\nCompare the performance of your neural network to some classical machine learning methods. Does this dataset / problem merit \"deep\" learning? Why or why not?\n\n## Bonus Challenge #2\nFeature importance: query your best neural network to see which features were the best predictors of income. Which ones were the best predictors?\n\n## Bonus Challenge #3\nModify your trained network to predict salary as a regression problem.","metadata":{}}]}