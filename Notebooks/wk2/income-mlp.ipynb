{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MLPs using US Census Income Data\n",
    "This week, we'll use a simple MLP to predict whether an individual makes over or under $50,000 per year based on some demographic characteristics. This notebook was created largely from [this one](https://www.kaggle.com/code/dogukantabak/income-prediction-pytorch). The dataset we'll use is on Kaggle [here](https://www.kaggle.com/datasets/jainaru/adult-income-census-dataset/data).  \n",
    "\n",
    "Learning outcomes:\n",
    "1. Pull in data on Kaggle\n",
    "2. Inspect and explore the data\n",
    "3. Split the dataset into train/test\n",
    "4. Train classical machine learning models on the data\n",
    "5. Build and train simple ANN with Pytorch for classification\n",
    "\n",
    "<img src=\"https://isaiahlg.com/portfolio/csci5922/mod2/nn1.png\" alt=\"Basic MLP\" style=\"width:37%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Income Predictor Dataset - US Adult\n",
    "Link: https://www.kaggle.com/datasets/jainaru/adult-income-census-dataset/data\n",
    "\n",
    "The Adult Census Income dataset, extracted from the 1994 US Census Database by Barry Becker, serves as a valuable resource for understanding the intricate interplay between socio-economic factors and income levels. Comprising anonymized information such as occupation, age, native country, race, capital gain, capital loss, education, work class, and more, this dataset offers a comprehensive view of the American demographic landscape.\n",
    "\n",
    "**Dataset Overview**\n",
    "The dataset consists of two CSV files: adult-training.txt and adult-test.txt, each row representing an individual. Key features include occupation, age, native country, race, capital gain, capital loss, education, work class, and more. The target variable, 'income_bracket', categorizes individuals into two groups: \">50K\" and \"<=50K\".\n",
    "\n",
    "**Exploration and Preprocessing**\n",
    "Exploring the dataset reveals a mix of categorical and continuous features, as well as missing values. Understanding the distribution and relationships of these features is crucial for feature selection and data preprocessing, including handling missing values and encoding categorical variables.\n",
    "\n",
    "**Modeling and Evaluation**\n",
    "To predict income levels, various classifiers can be trained on the training dataset and evaluated using the test dataset. Algorithms such as logistic regression, decision trees, random forests, and neural networks can be employed based on the dataset's complexity and the desired performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data into Kaggle Notebooks\n",
    "We want our income data into this notebook. To do this:\n",
    "- Go to the \"Input\" section on the right\n",
    "- Click \"Add Input\"\n",
    "- Click \"Datasets\" to search for just datasets\n",
    "- Search \"Income Predictor\"\n",
    "- Import the first result called \"Income Predictor Dataset- US Adult\" by clicking the +\n",
    "The dataset should appear in the right-hand menu under datasets.\n",
    "\n",
    "Note: this notebook can be run with the standard CPU since the dataset is quite simple. We also shouldn't need \"internet on\" for this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/kaggle/input/adult-income-census-dataset/adult.csv') # read in the file\n",
    "data.head() # look at the first few rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.info() # look at the various columns and their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# look for null values and duplicate rows\n",
    "print(\"Null values by column:\\n\", data.isnull().sum())\n",
    "print(\"Number of duplicate rows: \", data.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# clean the data and reinspect\n",
    "data.drop_duplicates(inplace=True) # drop the duplicate rows\n",
    "data.replace('?', np.nan, inplace=True) # replace any values with a ? with \"NaN\" or \"Not a Number\"\n",
    "data.dropna(inplace=True) # drop any rows that have NA values\n",
    "data.info() # inspect data again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've dropped all duplicate rows and all rows with ? or null values, next we need to convert our variables with categorical answers to numeric values, extract our label, and scale our numeric values. For this, we'll use the classical machine learning library `sci-kit-learn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the categorical features\n",
    "categorical_features = ['workclass', 'education', 'marital.status', 'occupation', \n",
    "                        'relationship', 'race', 'sex', 'native.country', 'income']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# extract the label column\n",
    "X = data.drop('income', axis=1)\n",
    "y = data['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# scale the numeric features to each have a mean of 0, std dev of 1\n",
    "continuous_features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "scaler = StandardScaler()\n",
    "X[continuous_features] = scaler.fit_transform(X[continuous_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# split the data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch & Tensors\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors are also optimized for automatic differentiation (we’ll see more about that later in the Autograd section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API.\n",
    "\n",
    "Learn more here: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "\n",
    "To start let's import the libraries we need from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# convert test and training data to a tensor\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & Dataloaders\n",
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "Learn more here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create a TensorDataset within Pytorch\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# wrap the Dataset in a DataLoader to be iterable\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Device for Training\n",
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let’s check to see if torch.cuda or torch.backends.mps are available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Machine Learning Model\n",
    "\n",
    "Here's the fun part, building the machine learning model! Pytorch makes this relatively straightforward. Let's start with a very simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# figure out the width of the input tensor\n",
    "print('The shape of the X_train_tensor is:', X_train_tensor.shape)\n",
    "# let's use the second value, the # of columns\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "\n",
    "# instantiate the model\n",
    "model = NeuralNetwork(input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Neural Network\n",
    "\n",
    "First step is to define our training parameters. The three key ones are:\n",
    "1. Loss Function (https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "2. Learning Rate (https://www.geeksforgeeks.org/impact-of-learning-rate-on-a-model/)\n",
    "3. Optimizer (https://pytorch.org/docs/stable/optim.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# define a loss function\n",
    "criterion = nn.CrossEntropyLoss() # a go to for classication problems\n",
    "learning_rate = 0.001 # a standard starting point, use factors of 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam Optimizer: https://arxiv.org/abs/1412.6980"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for the training loop. The following code will define how many times we want to loop over the training data, and then executes that loop, running the data through the model with each batch, calculating the loss, and updating the model parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set the number of times to iterate over the entire training dataset\n",
    "num_epochs = 10\n",
    "\n",
    "# Start the training loop, iterating through the dataset `num_epochs` times\n",
    "for epoch in range(num_epochs):  # Loop over each epoch\n",
    "    model.train()  # Put the model into training mode (enables features like dropout)\n",
    "    running_loss = 0.0  # Initialize a variable to keep track of cumulative loss for the epoch\n",
    "\n",
    "    # Loop through each batch of data in the training dataset\n",
    "    for inputs, labels in train_loader:  # `inputs` are the features, `labels` are the targets\n",
    "        optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "        \n",
    "        outputs = model(inputs)  # Perform a forward pass through the model to get predictions\n",
    "        loss = criterion(outputs, labels)  # Compute the loss between predictions and actual labels\n",
    "        loss.backward()  # Perform backpropagation to calculate gradients of loss with respect to parameters\n",
    "        optimizer.step()  # Update model parameters based on the gradients\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate the loss for this batch\n",
    "    \n",
    "    # Calculate the average loss for this epoch\n",
    "    avg_loss = running_loss / len(train_loader)  # Divide total loss by the number of batches\n",
    "    # Print progress, showing the current epoch and average loss for the epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "The code below runs the test data through our trained model, and reports on the performance. Remember, the model did not see this data in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()  # Put the model in evaluation mode (disables features like dropout and gradient tracking)\n",
    "correct = 0  # Initialize a counter for correctly classified samples\n",
    "total = 0  # Initialize a counter for the total number of samples\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency and to save memory\n",
    "    for inputs, labels in test_loader:  # Loop through each batch in the test dataset\n",
    "        outputs = model(inputs)  # Perform a forward pass through the model to get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability for each sample\n",
    "        total += labels.size(0)  # Update the total count with the number of samples in this batch\n",
    "        correct += (predicted == labels).sum().item()  # Increment the correct count for accurate predictions\n",
    "\n",
    "accuracy = 100 * correct / total  # Calculate accuracy as a percentage\n",
    "print(f'Accuracy on test data: {accuracy:.2f}%')  # Display the accuracy of the model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading our Model\n",
    "A common way to save a model is to serialize the internal state dictionary (containing the model parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The process for loading a model includes re-creating the model structure and loading the state dictionary into it\n",
    "model = NeuralNetwork(input_dim)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This model can now be used to make predictions.\n",
    "classes = [\"Over $50k\", \"Under $50k\"] # is this correct, or should it be switched?\n",
    "row_index_to_test = 6\n",
    "\n",
    "# evaluate the model on this one item from the dataset\n",
    "model.eval()\n",
    "x, y = test_dataset[row_index_to_test][0], test_dataset[row_index_to_test][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "Write your answers to the following questions in a markdown cell at the end of your notebook.\n",
    "1. Play with the MLP model. Consider adding more layers, changing the size of layers, adding things like drop out, etc. Full list of types is here: torch.nn. What model architecture worked best for you?\n",
    "2. Play with the optimization parameters. Try other learning rates, more or less epochs, different loss functions, optimizers, etc. See which one gets a good result fastest. What model parameters worked best for you?\n",
    "3. Reflect on the question, what are some ethical considerations for building a model that classifies people as high or low earners based on their demographics?\n",
    "\n",
    "## Bonus Challenge #1\n",
    "Compare the performance of your neural network to some classical machine learning methods. Does this dataset / problem merit \"deep\" learning? Why or why not?\n",
    "\n",
    "## Bonus Challenge #2\n",
    "Feature importance: query your best neural network to see which features were the best predictors of income. Which ones were the best predictors?\n",
    "\n",
    "## Bonus Challenge #3\n",
    "Modify your trained network to predict salary as a regression problem.\n",
    "\n",
    "## Deliverables\n",
    "1. Submit the Kaggle URL of your notebook with the cells run to the Discord channel #notebook-submissions. \n",
    "2. Find a Python course on DataCamp or platform of your choice that is 6-20 hours long on a topic you are interested that you'll commit to yourself to take over the next 10 weeks. Share a link to the course in the Discord channel #learning-python. You won't be graded on completing the course, this is just for your own benefit and mutual accountability.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4979622,
     "sourceId": 8375157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
