{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Cookie for authentication\n",
    "COOKIE = {\n",
    "    \"csrftoken\": \"I9x2jvGGE4se3MBa9moavDtC9o8YEgaA4Rup5ijhHJjCTRn0qRpHGJW06XG0SooG\",\n",
    "    \"sessionid\": \"y44nlmh7rjrqvvxj902jc8pmw918m1p7\",\n",
    "}\n",
    "\n",
    "# Base URL for file downloads\n",
    "BASE_URL = \"https://api.mosaiks.org/portal/download_grid_file/coarsened_global_dense_grid_decimal_place=1_GHS_pop_weight=True\"\n",
    "\n",
    "# Destination directory\n",
    "DEST_DIR = \"/Users/isaiah/code/ai4good/data/wk5/mosaiks001\"\n",
    "os.makedirs(DEST_DIR, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# Regions and chunks to download\n",
    "regions = {\n",
    "    # \"Africa\": [1, 2, 3],\n",
    "    # \"Asia\": [1, 2, 3, 4, 5, 6],\n",
    "    # \"Europe\": [1, 2],\n",
    "    # \"North America\": [1, 2, 3],\n",
    "    \"Oceania\": [1],\n",
    "    # \"South America\": [1, 2],\n",
    "    \"Australia\": [1],\n",
    "}\n",
    "\n",
    "# Function to download a file with progress bar\n",
    "def download_file(url, filename):\n",
    "    response = requests.get(url, cookies=COOKIE, stream=True, verify=False)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        total_size = int(response.headers.get(\"content-length\", 0))\n",
    "        with open(filename, \"wb\") as f, tqdm(\n",
    "            desc=os.path.basename(filename),\n",
    "            total=total_size,\n",
    "            unit=\"B\",\n",
    "            unit_scale=True,\n",
    "            unit_divisor=1024,\n",
    "        ) as bar:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "                bar.update(len(chunk))\n",
    "        print(f\"‚úÖ Downloaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to download {url} (Status {response.status_code})\")\n",
    "\n",
    "# Download each file\n",
    "for region, chunks in regions.items():\n",
    "    for chunk in chunks:\n",
    "        filename = os.path.join(DEST_DIR, f\"{region.lower()}_{chunk}.zip\")\n",
    "        url = f\"{BASE_URL}/coarsened_global_dense_grid_decimal_place=1_GHS_pop_weight=True_{region}_chunk={chunk}.zip\"\n",
    "        if (region == \"Australia\" or region == \"Oceania\"):\n",
    "            url = f\"{BASE_URL}/coarsened_global_dense_grid_decimal_place=1_GHS_pop_weight=True_{region}.zip\"\n",
    "        \n",
    "        print(f\"üì• Downloading {url}...\")\n",
    "        download_file(url, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the files\n",
    "import zipfile\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory containing the zip files\n",
    "zip_dir = \"/Users/isaiah/code/ai4good/data/wk5/mosaiks100\"\n",
    "# Define the directory to extract the contents\n",
    "extract_dir = \"/Users/isaiah/code/ai4good/data/wk5/mosaiks100/\"\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "# Loop through all zip files in the directory\n",
    "for zip_file in glob.glob(os.path.join(zip_dir, \"*.zip\")):\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as z:\n",
    "        # Extract all the contents into the extraction directory\n",
    "        z.extractall(extract_dir)\n",
    "        print(f\"Extracted {zip_file} to {extract_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the csv files into one for each continent\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory containing the CSV files\n",
    "csv_dir = \"/Users/isaiah/code/ai4good/data/wk5/mosaiks001/\"\n",
    "# Define the output directory for the combined CSV files\n",
    "output_dir = \"/Users/isaiah/code/ai4good/data/wk5/mosaiks001/\"\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Loop through each continent\n",
    "for continent in [\"Europe\", \"North_America\", \"South_America\", \"Oceania\", \"Asia\"]:\n",
    "    print(f\"Combining {continent} files...\")\n",
    "    # Find all CSV files for the continent\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, f\"*{continent}_*.csv\"))\n",
    "    # Combine the CSV files into one DataFrame\n",
    "    print(f\"Combining {len(csv_files)} files...\")\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in csv_files], ignore_index=True)\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df.to_csv(os.path.join(output_dir, f\"{continent}.csv\"), index=False)\n",
    "    print(f\"Combined {len(csv_files)} files into {continent}.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
