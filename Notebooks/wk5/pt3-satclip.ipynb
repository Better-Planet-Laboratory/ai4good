{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngz8zz9Gvbxh"
   },
   "source": [
    "# Predicting Wealth & Poverty with SatCLIP\n",
    "\n",
    "SatCLIP is a model that was trained on 1.5B geotagged images from the internet. It uses a contrastive learning approach to learn representations of satellite imagery. You can read more about it in the [paper](https://arxiv.org/abs/2311.17179) or check out the [GitHub repo](https://github.com/microsoft/satclip/). Here, we'll use it to extract location encodings and predict wealth and poverty in SustainBench Task 1A.\n",
    "\n",
    "![satclip](https://raw.githubusercontent.com/microsoft/satclip/master/figures/satclip.png)\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "We start by setting up **SatCLIP** code and installing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:21:46.133053Z",
     "iopub.status.busy": "2025-02-10T01:21:46.132722Z",
     "iopub.status.idle": "2025-02-10T01:21:47.850581Z",
     "shell.execute_reply": "2025-02-10T01:21:47.849548Z",
     "shell.execute_reply.started": "2025-02-10T01:21:46.133026Z"
    },
    "id": "tD7wze7andRh",
    "outputId": "b4949082-62e9-474b-f62d-fd234982d03d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/microsoft/satclip.git # Clone SatCLIP repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find the SatCLIP repo in the working directory now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:21:47.852150Z",
     "iopub.status.busy": "2025-02-10T01:21:47.851887Z",
     "iopub.status.idle": "2025-02-10T01:22:16.396583Z",
     "shell.execute_reply": "2025-02-10T01:22:16.395749Z",
     "shell.execute_reply.started": "2025-02-10T01:21:47.852129Z"
    },
    "id": "Q72Ypu0Cr3Sc",
    "outputId": "9e922bc4-c78d-4a24-9ed8-f9f6616445c8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# install packages we need to run SatCLIP\n",
    "!pip install lightning --quiet\n",
    "!pip install rasterio --quiet\n",
    "!pip install torchgeo --quiet\n",
    "!pip install basemap --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcZKqU9wOTk"
   },
   "source": [
    "Chose a SatCLIP model from the list of available pretrained models [here](https://github.com/microsoft/satclip#pretrained-models). They all perform somewhat similarly. Let's download a SatCLIP using a ResNet18 vision encoder and $L=40$ Legendre polynomials in the location encoder (i.e., a high-resolution SatCLIP). Other options include using a ResNet50 vision encoder or a Vision Transformer (ViT) encoder, or using $L=10$ Legendre polynomials (i.e., a low-resolution SatCLIP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:16.398441Z",
     "iopub.status.busy": "2025-02-10T01:22:16.398210Z",
     "iopub.status.idle": "2025-02-10T01:22:34.437153Z",
     "shell.execute_reply": "2025-02-10T01:22:34.436384Z",
     "shell.execute_reply.started": "2025-02-10T01:22:16.398421Z"
    },
    "id": "grEIwoFjoHvu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import code from satclip repo\n",
    "import sys\n",
    "sys.path.append('./satclip/satclip')\n",
    "import torch\n",
    "from load import get_satclip\n",
    "\n",
    "# data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# for prediction\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "\n",
    "# configure pytorch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") # CUDA for GPUs\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Metal Performance Shaders (MPS) for macOS\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # Otherwise use CPUs\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# configure random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_FUpXihwwpB"
   },
   "source": [
    "Load required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring in Poverty Mapping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:34.439098Z",
     "iopub.status.busy": "2025-02-10T01:22:34.438580Z",
     "iopub.status.idle": "2025-02-10T01:22:34.444783Z",
     "shell.execute_reply": "2025-02-10T01:22:34.443943Z",
     "shell.execute_reply.started": "2025-02-10T01:22:34.439071Z"
    },
    "id": "fvNnJtpEq31R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_poverty_data(path=\"dhs_final_labels.csv\", pred=\"asset_index\"):\n",
    "  '''\n",
    "  Download and process the poverty mapping dataset from SustainBench (more info: https://www.nature.com/articles/sdata2018246)\n",
    "\n",
    "  Parameters:\n",
    "  pred = numeric; name of outcome variable to be returned, 0-indexed. Must be numeric column. 6 is 'asset_index'\n",
    "  norm_y = logical; should outcome be normalized\n",
    "\n",
    "  Return:\n",
    "  coords = spatial coordinates (lon/lat)\n",
    "  y = outcome variable\n",
    "  '''\n",
    "  df = pd.read_csv(path)\n",
    "  lat = np.array(df['lat'])\n",
    "  lon = np.array(df['lon'])\n",
    "  coords = np.column_stack((lon, lat))\n",
    "  y = np.array(df[pred]).reshape(-1)\n",
    "\n",
    "  return torch.tensor(coords), torch.tensor(y)\n",
    "\n",
    "print('function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:34.445843Z",
     "iopub.status.busy": "2025-02-10T01:22:34.445599Z",
     "iopub.status.idle": "2025-02-10T01:22:34.981398Z",
     "shell.execute_reply": "2025-02-10T01:22:34.980586Z",
     "shell.execute_reply.started": "2025-02-10T01:22:34.445822Z"
    },
    "id": "8QFfrQUysNZ2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "coords, y = get_poverty_data(pred=\"asset_index\", path='/kaggle/input/dhs-labels-for-poverty-mapping-from-sustainbench/dhs_final_labels.csv')\n",
    "print(\"Coordinates tensor:\", coords)\n",
    "print(\"Outcome variable tensor:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGR2jSZLsiUx"
   },
   "source": [
    "Let's plot our data. Here we show a map of the world with our locations colored by mean temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:34.982520Z",
     "iopub.status.busy": "2025-02-10T01:22:34.982257Z",
     "iopub.status.idle": "2025-02-10T01:22:41.771788Z",
     "shell.execute_reply": "2025-02-10T01:22:41.770897Z",
     "shell.execute_reply.started": "2025-02-10T01:22:34.982486Z"
    },
    "id": "hmZmDOXPst0y",
    "outputId": "8f3338c8-556a-4937-fa50-bc54d236e6b3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(5, 3))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "ax.scatter(coords[:,0], coords[:,1], c=y, s=5)\n",
    "ax.set_title('Mean Asset Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHUQyTODt1UX"
   },
   "source": [
    "## Get SatCLIP Embeddings\n",
    "\n",
    "We now want to wealth values using SatCLIP embeddings. First, we need to obtain the location embeddings for our dataset from the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:41.773036Z",
     "iopub.status.busy": "2025-02-10T01:22:41.772723Z",
     "iopub.status.idle": "2025-02-10T01:22:43.174516Z",
     "shell.execute_reply": "2025-02-10T01:22:43.173493Z",
     "shell.execute_reply.started": "2025-02-10T01:22:41.773007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# download the pretrained model weights to working directory, should be around 70MB\n",
    "!wget 'https://satclip.z13.web.core.windows.net/satclip/satclip-resnet18-l40.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:43.177469Z",
     "iopub.status.busy": "2025-02-10T01:22:43.177200Z",
     "iopub.status.idle": "2025-02-10T01:22:45.544199Z",
     "shell.execute_reply": "2025-02-10T01:22:45.543263Z",
     "shell.execute_reply.started": "2025-02-10T01:22:43.177446Z"
    },
    "id": "AFkpE1d7q9yW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "satclip_path = 'satclip-resnet18-l40.ckpt'\n",
    "\n",
    "satclip_model = get_satclip(satclip_path, device=device) # Only loads location encoder by default\n",
    "satclip_model.eval()\n",
    "with torch.no_grad():\n",
    "  x = satclip_model(coords.double().to(device)).detach().cpu()\n",
    "\n",
    "print(\"Shape of coordinate tensor:\", coords.shape)\n",
    "print(\"Shape of location embeddings tensor:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaNU4D96us9O"
   },
   "source": [
    "We have now collected a 256-dimensional location embedding for each latitude/longitude coordinate in our dataset. Let's merge it, convert it to a dataframe, and save it to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:22:45.546446Z",
     "iopub.status.busy": "2025-02-10T01:22:45.546201Z",
     "iopub.status.idle": "2025-02-10T01:23:23.008737Z",
     "shell.execute_reply": "2025-02-10T01:23:23.008018Z",
     "shell.execute_reply.started": "2025-02-10T01:22:45.546426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# convert x, y, and coords from tensors in dataframes\n",
    "df = pd.DataFrame(x.numpy(), columns=[f\"X_{i}\" for i in range(x.shape[1])])\n",
    "df[\"asset_index\"] = y.numpy()  # Ensure y is a 1D column\n",
    "df[\"lon\"] = coords[:, 0].numpy()  # Extract longitude\n",
    "df[\"lat\"] = coords[:, 1].numpy()  # Extract latitude\n",
    "df.head()\n",
    "df.to_csv(\"dhs_final_labels_with_satclip.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj1Hqqtru51Y"
   },
   "source": [
    "## Prep Data for Predicting Wealth Using Regression\n",
    "1. Drop any rows with missing label values\n",
    "2. Split into test / validation / train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:23:23.009868Z",
     "iopub.status.busy": "2025-02-10T01:23:23.009611Z",
     "iopub.status.idle": "2025-02-10T01:23:58.355968Z",
     "shell.execute_reply": "2025-02-10T01:23:58.355167Z",
     "shell.execute_reply.started": "2025-02-10T01:23:23.009846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read back in the dataframe\n",
    "df = pd.read_csv(\"dhs_final_labels_with_satclip.csv\")\n",
    "\n",
    "# check for NaNs\n",
    "print(\"Number of NaNs in df: \", df.isna().sum())\n",
    "# drop rows with NaNs\n",
    "df = df.dropna()\n",
    "# check the shape again\n",
    "print(\"Shape of df: \", df.shape)\n",
    "\n",
    "# create a train/val/test split of 80/10/10\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train and test sets\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# split the test set into val and test sets\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
    "# check the shapes of the data\n",
    "print(\"Shape of df_train: \", df_train.shape)\n",
    "print(\"Shape of df_val: \", df_val.shape)\n",
    "print(\"Shape of df_test: \", df_test.shape)\n",
    "\n",
    "# save dfs as csvs\n",
    "df_train.to_csv(\"df_train.csv\", index=False)\n",
    "df_val.to_csv(\"df_val.csv\", index=False)\n",
    "df_test.to_csv(\"df_test.csv\", index=False)\n",
    "\n",
    "print(\"files written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split up each of the datasets into inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:23:58.357176Z",
     "iopub.status.busy": "2025-02-10T01:23:58.356834Z",
     "iopub.status.idle": "2025-02-10T01:24:03.086103Z",
     "shell.execute_reply": "2025-02-10T01:24:03.085125Z",
     "shell.execute_reply.started": "2025-02-10T01:23:58.357146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get data again\n",
    "df_train = pd.read_csv(\"df_train.csv\")\n",
    "df_val = pd.read_csv(\"df_val.csv\")\n",
    "df_test = pd.read_csv(\"df_val.csv\")\n",
    "\n",
    "# get label y\n",
    "y_train = df_train[\"asset_index\"]\n",
    "y_val = df_val[\"asset_index\"]\n",
    "y_test = df_test[\"asset_index\"]\n",
    "\n",
    "# get coordinates\n",
    "coords_train = df_train[[\"lon\", \"lat\"]]\n",
    "coords_val = df_val[[\"lon\", \"lat\"]]\n",
    "coords_test = df_test[[\"lon\", \"lat\"]]\n",
    "\n",
    "# x is all the data in columns with names formatted like \"X_1\"\n",
    "X_train = df_train[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "X_val = df_val[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "X_test = df_test[[col for col in df.columns if col.startswith(\"X_\")]]\n",
    "\n",
    "print(\"data separated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Using Regression with Ridge Model\n",
    "Let's repeat the analysis we did with MOSAIKS embeddings with our SatCLIP embeddings. Start by reading in our dataset with the embeddings, removing any NA rows, and splitting it 80/10/10 train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:24:03.087582Z",
     "iopub.status.busy": "2025-02-10T01:24:03.087230Z",
     "iopub.status.idle": "2025-02-10T01:24:13.338917Z",
     "shell.execute_reply": "2025-02-10T01:24:13.337923Z",
     "shell.execute_reply.started": "2025-02-10T01:24:03.087548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define alpha values to loop over\n",
    "alpha_values = [10**i for i in range(-5, 4)]  # Corrected alpha values from 0.001 to 1000\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    y_pred = ridge_model.predict(X_val)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "    # Create scatter plot using Seaborn\n",
    "    sns.scatterplot(x=y_val, y=y_pred, ax=axes[i], s=10)\n",
    "    axes[i].plot([-4, 4], [-4, 4], \"r--\")  # 45-degree line\n",
    "    \n",
    "    # Fit a regression line with confidence interval\n",
    "    sns.regplot(x=y_val, y=y_pred, ax=axes[i], scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "    \n",
    "    axes[i].set_title(f\"Validation Performance, Alpha={alpha}, R²={r2:.6f}\")\n",
    "    axes[i].set_xlabel(\"Asset Index (True)\")\n",
    "    axes[i].set_ylabel(\"Asset Index (Predicted)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that alpha values below 0.01 perform similarly well. Let's choose the same alpha value as MOSAIKS, alpha 0.01 to run on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:24:13.340252Z",
     "iopub.status.busy": "2025-02-10T01:24:13.339826Z",
     "iopub.status.idle": "2025-02-10T01:24:14.511134Z",
     "shell.execute_reply": "2025-02-10T01:24:14.510256Z",
     "shell.execute_reply.started": "2025-02-10T01:24:13.340213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select the best alpha value from the validation data\n",
    "alpha = 0.01\n",
    "\n",
    "# Train Ridge regression model on training data\n",
    "ridge_model = Ridge(alpha=alpha)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "# Use coefficients from training for inference on test data\n",
    "y_pred = X_test @ coefficients + ridge_model.intercept_\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Create scatter plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, s=10)\n",
    "plt.plot([-4, 4], [-4, 4], \"r--\")  # 45-degree line\n",
    "\n",
    "# Fit a regression line with confidence interval\n",
    "sns.regplot(x=y_test, y=y_pred, scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "\n",
    "plt.title(f\"Test Data Peformance, Alpha={alpha}, R²={r2:.4f}\")\n",
    "plt.xlabel(\"Asset Index (True)\")\n",
    "plt.ylabel(\"Asset Index (Predicted)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Our final performance on the test data is slightly better than the validation data with an R-squared of 0.5075. How does this compare to MOSAIKS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Using a Simple MLP\n",
    "Instead of just regression, we can use any prediction model to predict our outcome variable from the location embeddings. Here, let's implement a very simple MLP to predict poverty from the SatCLIP location embeddings. For a more task specific, \"heavier\" approach, you can also directly unfreeze the location encoder `model` above and fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:24:14.512306Z",
     "iopub.status.busy": "2025-02-10T01:24:14.512011Z",
     "iopub.status.idle": "2025-02-10T01:24:20.825753Z",
     "shell.execute_reply": "2025-02-10T01:24:20.824771Z",
     "shell.execute_reply.started": "2025-02-10T01:24:14.512280Z"
    },
    "id": "z0N5oeZ9u0sV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read in merged dataset\n",
    "df = pd.read_csv(\"dhs_final_labels_with_satclip.csv\")\n",
    "\n",
    "# Convert lat/lon back to tensors\n",
    "coords_train = torch.tensor(df_train[[\"lon\", \"lat\"]].values, dtype=torch.float32)\n",
    "coords_val = torch.tensor(df_val[[\"lon\", \"lat\"]].values, dtype=torch.float32)\n",
    "coords_test = torch.tensor(df_test[[\"lon\", \"lat\"]].values, dtype=torch.float32)\n",
    "\n",
    "# Convert features (X) and target (y) back to tensors\n",
    "X_train = torch.tensor(df_train.filter(like=\"X_\").values, dtype=torch.float32)\n",
    "X_val = torch.tensor(df_val.filter(like=\"X_\").values, dtype=torch.float32)\n",
    "X_test = torch.tensor(df_test.filter(like=\"X_\").values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(df_train[\"asset_index\"].values, dtype=torch.float32)\n",
    "y_val = torch.tensor(df_val[\"asset_index\"].values, dtype=torch.float32)\n",
    "y_test = torch.tensor(df_test[\"asset_index\"].values, dtype=torch.float32)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"coords_train shape: {coords_train.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:24:20.826916Z",
     "iopub.status.busy": "2025-02-10T01:24:20.826640Z",
     "iopub.status.idle": "2025-02-10T01:24:20.833172Z",
     "shell.execute_reply": "2025-02-10T01:24:20.832167Z",
     "shell.execute_reply.started": "2025-02-10T01:24:20.826894Z"
    },
    "id": "FW65kkGexL9U",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, dim_hidden, num_layers, out_dims):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Linear(input_dim, dim_hidden, bias=True), nn.ReLU()] # Input layer\n",
    "        layers += [nn.Linear(dim_hidden, dim_hidden, bias=True), nn.ReLU()] * num_layers # Hidden layers\n",
    "        layers += [nn.Linear(dim_hidden, out_dims, bias=True)] # Output layer\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "print('model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:24:20.834407Z",
     "iopub.status.busy": "2025-02-10T01:24:20.834113Z",
     "iopub.status.idle": "2025-02-10T01:24:20.852247Z",
     "shell.execute_reply": "2025-02-10T01:24:20.851321Z",
     "shell.execute_reply.started": "2025-02-10T01:24:20.834384Z"
    },
    "id": "CfsgJLJuqEtv",
    "outputId": "11c13a15-e73a-4683-c0de-4a90cb827650",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "satclip_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufbqVygqxpOq"
   },
   "source": [
    "Let's set up and run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:24:20.853428Z",
     "iopub.status.busy": "2025-02-10T01:24:20.853151Z",
     "iopub.status.idle": "2025-02-10T01:30:17.482769Z",
     "shell.execute_reply": "2025-02-10T01:30:17.481977Z",
     "shell.execute_reply.started": "2025-02-10T01:24:20.853399Z"
    },
    "id": "di_SelDzxkjD",
    "outputId": "e9803126-e726-4d5d-e391-1b8281b810d8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model, loss, optimizer\n",
    "pred_model = MLP(input_dim=256, dim_hidden=64, num_layers=1, out_dims=1).float().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(pred_model.parameters(), lr=0.0001)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "epochs = 20000\n",
    "m = 500  # Evaluate every 'm' epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass on training data\n",
    "    y_pred = pred_model(X_train.float().to(device))\n",
    "    loss = criterion(y_pred.reshape(-1), y_train.float().to(device))\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track training loss\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # Evaluate on validation set every m epochs\n",
    "    if (epoch + 1) % m == 0:\n",
    "        pred_model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = pred_model(X_val.float().to(device))\n",
    "            val_loss = criterion(y_val_pred.reshape(-1), y_val.float().to(device)).item()\n",
    "            val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        pred_model.train()  # Set model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:30:17.484099Z",
     "iopub.status.busy": "2025-02-10T01:30:17.483742Z",
     "iopub.status.idle": "2025-02-10T01:30:17.490395Z",
     "shell.execute_reply": "2025-02-10T01:30:17.489647Z",
     "shell.execute_reply.started": "2025-02-10T01:30:17.484062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred_model_path = 'pred_model_weights_hl1_lr1e-4_e20k.pth'\n",
    "torch.save(pred_model.state_dict(), pred_model_path)\n",
    "print(\"model saved to\", pred_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkViMvRD0JsJ"
   },
   "source": [
    "Now let's make predictions on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:30:17.491338Z",
     "iopub.status.busy": "2025-02-10T01:30:17.491154Z",
     "iopub.status.idle": "2025-02-10T01:30:17.505680Z",
     "shell.execute_reply": "2025-02-10T01:30:17.504990Z",
     "shell.execute_reply.started": "2025-02-10T01:30:17.491322Z"
    },
    "id": "OIqIGJc7zPTe",
    "outputId": "6f22edbb-cbae-4979-b7a5-f1a5874ad55a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  pred_model.eval()\n",
    "  y_pred_test = pred_model(X_test.float().to(device))\n",
    "\n",
    "# Print test loss\n",
    "print(f'Test loss: {criterion(y_pred_test.reshape(-1), y_test.float().to(device)).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:30:17.506816Z",
     "iopub.status.busy": "2025-02-10T01:30:17.506515Z",
     "iopub.status.idle": "2025-02-10T01:30:18.499021Z",
     "shell.execute_reply": "2025-02-10T01:30:18.498150Z",
     "shell.execute_reply.started": "2025-02-10T01:30:17.506783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute test predictions\n",
    "with torch.no_grad():\n",
    "    pred_model.eval()\n",
    "    y_pred_test = pred_model(X_test.float().to(device)).cpu().numpy().flatten()\n",
    "\n",
    "# Convert y_test to NumPy\n",
    "y_test_np = y_test.cpu().numpy().flatten()\n",
    "\n",
    "# Compute R² score\n",
    "r2 = r2_score(y_test_np, y_pred_test)\n",
    "\n",
    "# Create scatter plot using Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x=y_test_np, y=y_pred_test, s=10)\n",
    "plt.plot([-4, 4], [-4, 4], \"r--\", label=\"45-degree line\")  # Perfect prediction line\n",
    "\n",
    "# Fit a regression line with confidence interval\n",
    "sns.regplot(x=y_test_np, y=y_pred_test, scatter=False, ci=95, line_kws={\"color\": \"blue\"})\n",
    "\n",
    "# Plot settings\n",
    "plt.title(f\"Test Data Performance, R²={r2:.2f}\")\n",
    "plt.xlabel(\"Asset Index (True)\")\n",
    "plt.ylabel(\"Asset Index (Predicted)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0l8MtgQ1w7i"
   },
   "source": [
    "Let's show the results on a map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:30:18.500189Z",
     "iopub.status.busy": "2025-02-10T01:30:18.499896Z",
     "iopub.status.idle": "2025-02-10T01:30:20.050193Z",
     "shell.execute_reply": "2025-02-10T01:30:20.049172Z",
     "shell.execute_reply.started": "2025-02-10T01:30:18.500155Z"
    },
    "id": "jMVfEZNS06IN",
    "outputId": "4eb5a9eb-0fe6-406b-fd58-2d9841e95280",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax[0])\n",
    "m.drawcoastlines()\n",
    "ax[0].scatter(coords_test[:,0], coords_test[:,1], c=y_test, s=5)\n",
    "ax[0].set_title('True')\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax[1])\n",
    "m.drawcoastlines()\n",
    "ax[1].scatter(coords_test[:,0], coords_test[:,1], c=y_pred_test.reshape(-1), s=5)\n",
    "ax[1].set_title('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Global Map of Wealth Predictions\n",
    "\n",
    "Now that we have a model that predicts wealth given any lat/long pair, let's run it over the whole world and see what it comes up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:30:20.051475Z",
     "iopub.status.busy": "2025-02-10T01:30:20.051140Z",
     "iopub.status.idle": "2025-02-10T01:30:20.057322Z",
     "shell.execute_reply": "2025-02-10T01:30:20.056362Z",
     "shell.execute_reply.started": "2025-02-10T01:30:20.051418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Latitude and longitude ranges\n",
    "lat_range = np.arange(-89.9, 90, 1)  # from -90 to +90 with 0.1 intervals\n",
    "lon_range = np.arange(-179.9, 180, 1)  # from -180 to +180 with 0.1 intervals\n",
    "\n",
    "# Create a grid of latitude and longitude pairs\n",
    "lat_grid, lon_grid = np.meshgrid(lat_range, lon_range)\n",
    "\n",
    "# Reshape into a 2D array of coordinate pairs (latitude, longitude)\n",
    "coords = np.column_stack((lon_grid.ravel(), lat_grid.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:43:32.946500Z",
     "iopub.status.busy": "2025-02-10T01:43:32.946096Z",
     "iopub.status.idle": "2025-02-10T01:43:32.967190Z",
     "shell.execute_reply": "2025-02-10T01:43:32.966412Z",
     "shell.execute_reply.started": "2025-02-10T01:43:32.946478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read in lat/lon coordinate pairs over land at 1 deg resolution\n",
    "df = pd.read_csv(\"/kaggle/input/dhs-labels-for-poverty-mapping-from-sustainbench/global_lat_lon_pairs.csv\")\n",
    "lon, lat = df['lon'].values, df['lat'].values\n",
    "coords = np.column_stack([lon, lat])\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "geo_points = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "print(geo_points)\n",
    "print(geo_points.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run these locations through the pre-trained SatCLIP location encoder to get location embeddings for all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:43:37.924373Z",
     "iopub.status.busy": "2025-02-10T01:43:37.924080Z",
     "iopub.status.idle": "2025-02-10T01:43:38.733238Z",
     "shell.execute_reply": "2025-02-10T01:43:38.732470Z",
     "shell.execute_reply.started": "2025-02-10T01:43:37.924351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "satclip_path = 'satclip-resnet18-l40.ckpt'\n",
    "\n",
    "satclip_model = get_satclip(satclip_path, device=device) # Only loads location encoder by default\n",
    "satclip_model.eval()\n",
    "with torch.no_grad():\n",
    "  X_global  = satclip_model(geo_points.double().to(device)).detach().cpu()\n",
    "\n",
    "print(\"Shape of coordinate tensor:\", coords.shape)\n",
    "print(\"Shape of location embeddings tensor:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T01:45:58.812519Z",
     "iopub.status.busy": "2025-02-10T01:45:58.812225Z",
     "iopub.status.idle": "2025-02-10T01:46:00.197717Z",
     "shell.execute_reply": "2025-02-10T01:46:00.196821Z",
     "shell.execute_reply.started": "2025-02-10T01:45:58.812499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reinstantiate the prediction model\n",
    "pred_model = MLP(input_dim=256, dim_hidden=64, num_layers=1, out_dims=1).float().to(device)\n",
    "# load the weights\n",
    "pred_model_path = 'pred_model_weights_hl1_lr1e-4_e20k.pth'\n",
    "pred_model.load_state_dict(torch.load(pred_model_path, weights_only=True))\n",
    "\n",
    "# run inference on the global dataset\n",
    "with torch.no_grad():\n",
    "  pred_model.eval()\n",
    "  y_pred_global = pred_model(X_global.float().to(device))\n",
    "\n",
    "# plot it globally\n",
    "fig, ax = plt.subplots(1, figsize=(10, 5))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "ax.scatter(geo_points[:,0], geo_points[:,1], c=y_pred_global.cpu(), s=5)\n",
    "ax.set_title('Mean Asset Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "1. Given the regression results above, answer the following questions:\n",
    "    - How did the MLP predictions do compared to the Ridge regression predictions? Why do you think this is?\n",
    "    - How did the SatCLIP embeddings perform at predicting wealth compared with the MOSAIKS embeddings? What do you think is the reason behind that?\n",
    "2. Given the gloabl prediction, how does this compare with your intuition of the global distribution of wealth and poverty? Do you think it is more, less, or equally (in)valid to use this model trained on SatCLIP embeddings and DHS clusters for a global level prediction than MOSAIKS?\n",
    "3. What modifications would you need to make to SatCLIP or MOSAIKS and this model to be able to predict a CHANGE in wealth and poverty over time? \n",
    "\n",
    "## Bonus Assignment (big one)\n",
    "Redo this wealth prediction task in a Kaggle Notebook using the [Clay model](https://clay-foundation.github.io/model/index.html). Note that you may need the imagery input from [SustainBench](https://clay-foundation.github.io/model/index.html) in order to extract embeddings for each of the points. It will not be trivial to get the development environment up and running. I'd suggest you start with trying to get this [example notebook](https://clay-foundation.github.io/model/tutorials/embeddings.html) on exploring embeddings from Clay running. Let the class know if you figure it out!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6635435,
     "sourceId": 10707414,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "satclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
