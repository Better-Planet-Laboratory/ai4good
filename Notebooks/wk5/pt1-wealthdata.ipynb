{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling and Visualizing Wealth & Poverty Data\n",
    "\n",
    "\"Despite decades of declining poverty rates, an estimated 8.4% of the global population remains in extreme poverty as of 2019, and progress has slowed in recent years [1]. But data on poverty remain surprisingly sparse, hampering efforts at monitoring local progress, targeting aid to those who need it, and evaluating the effectiveness of antipoverty programs [2]. Previous works [3,4] have demonstrated using computer vision on satellite images and street-level images to predict economic livelihood.\" [5]\n",
    "\n",
    "In this notebook, we will pull a 2021 benchmark dataset from the Stanford Sustainability and AI Lab called \"SustainBench\". This dataset contains a variety of datasets related to sustainability, including datasets related to poverty and wealth mapping. More info on it can be found on the [project website](https://sustainlab-group.github.io/sustainbench/), the [GitHub repo](https://github.com/sustainlab-group/sustainbench), or the [arXiv paper](https://arxiv.org/abs/2111.04724). The data comes from surveys collected by the [Demographic and Health Surveys (DHS) Program](https://dhsprogram.com/) from USAID (RIP üò¢). Nationally represenative surveys are conducted every few years in dozens of low- and middle-income countries (LMICs) around the world. Surveyors will go out to urban neigborhoods or rural communities and survey a few dozen random households within that \"cluster\". The anonymized household level data is geotagged with the coordinates of the cluster with a jitter to further protect privacy. The jitter is within a 2km radius for urban clusters, and a 5km radius for rural clusters. We will focus on Task 1A from SustainBench, mapping wealth and poverty spatially. SustainBench has made our lives easier by collating this data for 80k+ clusters and making it publicly avaiable, but you can request the original and latest household-level data directly from the [DHS on their website](https://dhsprogram.com/data/available-datasets.cfm), it takes just a couple of days to get approved.\n",
    "\n",
    "![sustainbench](https://sustainlab-group.github.io/sustainbench/assets/images/sdg1_summary.png)\n",
    "\n",
    "In subsequent notebooks, we then will pull in geospatial foundation models such as [SatCLIP](https://github.com/microsoft/satclip) and [MOSAIKS](https://www.mosaiks.org/). We will then use these models to extract features from the poverty and wealth mapping dataset, and then train a linear classifier on top of these features to predict the poverty and wealth labels. We will then evaluate the performance of each model on the test set.\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Identify benchmark datasets for a task of interest\n",
    "2. Pull in the relevant data\n",
    "3. Visualize it\n",
    "\n",
    "### References\n",
    "\n",
    "[1] United Nations Department of Economic and Social Affairs. The Sustainable Development Goals Report 2021. The Sustainable Development Goals Report. United Nations, 2021 edition, 2021. ISBN 978-92-1-005608-3. doi: 10.18356/9789210056083. URL https://www.un-ilibrary.org/content/books/9789210056083.\n",
    "\n",
    "[2] M. Burke, A. Driscoll, D. B. Lobell, and S. Ermon. Using satellite imagery to understand and promote sustainable development. Science, 371(6535), 2021. doi: 10.1126/science.448abe8628. URL https://www.science.org/doi/10.1126/science.abe8628.\n",
    "\n",
    "[3] C. Yeh, A. Perez, A. Driscoll, G. Azzari, Z. Tang, D. Lobell, S. Ermon, and M. Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in Africa. Nature Communications, 11(1), 5 2020. ISSN 2041-1723. doi: 10.1038/s41467-020-58916185-w. URL https://www.nature.com/articles/s41467-020-16185-w.\n",
    "\n",
    "[4] J. Lee, D. Grosz, B. Uzkent, S. Zeng, M. Burke, D. Lobell, and S. Ermon. Predicting Livelihood Indicators from Community-Generated Street-Level Imagery. Proceedings of the AAAI Conference on Artificial Intelligence, 35(1):268‚Äì276, 5 2021. ISSN 2374-3468. URL https://ojs.aaai.org/index.php/AAAI/article/view/16101.\n",
    "\n",
    "[5] C. Yeh, C. Meng, S. Wang, A. Driscoll, E. Rozi, P. Liu, J. Lee, M. Burke, D. Lobell, and S. Ermon, ‚ÄúSustainBench: Benchmarks for Monitoring the Sustainable Development Goals with Machine Learning,‚Äù in Thirty-fifth Conference on Neural Information Processing Systems, Datasets and Benchmarks Track (Round 2), Dec. 2021. [Online]. Available: https://openreview.net/forum?id=5HR3vCylqD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# install any libraries that are missing\n",
    "!pip install basemap --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data reading and manipulation\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "print(\"imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data\n",
    "The dataset we'll fetch is described [here](https://sustainlab-group.github.io/sustainbench/docs/datasets/dhs.html) and hosted on Google Drive [here](https://drive.google.com/drive/folders/1tzWDfd4Y5MvJnJb-lHieOuD-aVcUqzcu?usp=sharing). I've uploaded it to Kaggle [here](https://www.kaggle.com/datasets/isaiahlg/dhs-labels-for-poverty-mapping-from-sustainbench/) and pulled it into this notebook under Inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and Visualize Dataset\n",
    "Every good data scientists knows that you need to visualize your data to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read in the csv file\n",
    "# remove modify the path as needed\n",
    "labels_path = ('/kaggle/input/dhs_final_labels.csv')\n",
    "df = pd.read_csv(labels_path)\n",
    "\n",
    "# now convert this regular dataframe into a nifty geopandas dataframe\n",
    "# learn more about what a \"geo\" dataframe is here: https://geopandas.org/en/stable/docs/user_guide/data_structures.html#geodataframe\n",
    "# it's based on Python Shapely geometries: https://shapely.readthedocs.io/en/stable/geometry.html\n",
    "# which is in turn based on C/C++ GEOS geometries: https://libgeos.org/usage/\n",
    "# which is in turn based on the OGC Simple Features standard: https://en.wikipedia.org/wiki/Simple_Features\n",
    "# which describes well-known text (WKT) representations of vector geometry: https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry\n",
    "# fun!\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat), crs=\"EPSG:4326\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# I wanted visualize with lonboard from Development Seed built on top of deck.gl, but I couldn't get it working in Kaggle\n",
    "# learn more about it at https://github.com/developmentseed/lonboard or in the documentation here https://developmentseed.org/lonboard/latest/\n",
    "# Discussion about this issue here: https://github.com/developmentseed/lonboard/discussions/750\n",
    "# bonus if you can figure out an interactive viz in Kaggle!\n",
    "# instead we'll use a static visualization below:\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "m = Basemap(projection='cyl', resolution='c', ax=ax)\n",
    "m.drawcoastlines()\n",
    "ax.scatter(df['lon'], df['lat'], c=df['asset_index'], s=1)\n",
    "ax.set_title('Mean Asset Index Labels per DHS Cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "1. Answer the following questions: What are some countries missing from this dataset? Why do you think DHS didn't include them? Could this lead to potential biases? ü§î\n",
    "2. Visualize another variable from the labels file. Which one did you choose and why? How does the distribution compare to the asset index? Do you think there's a correlation between the two variables you chose.\n",
    "\n",
    "## Bonus Assignment\n",
    "Recreate the visualization above using an interactive geospatial Python visualization library (ie leaflet, carto, lonboard, etc etc..). Add your code below."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6635435,
     "sourceId": 10707414,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
