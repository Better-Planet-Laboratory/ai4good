{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6398691f93e37503",
   "metadata": {},
   "source": [
    "# Crop Classification from Sentinel-2 Satellite Imagery with XGBoosted Trees\n",
    "\n",
    "Agricultural monitoring is least developed for smallholders in low- and middle-income countries — communities most likely to be impacted by hunger, poverty, and climate change. Recent efforts to monitor smallholder productivity are limited in spatial and temporal scope.\n",
    "\n",
    "In this notebook, we provide an end-to-end machine learning pipeline built on satellite data from Google Earth Engine for high-resolution, wall-to-wall time series mapping of crop area and yield, demonstrated for maize at every 10-meter pixel in Rwanda over 2019-2023. Gradient boosted tree models are built from more than 60,000 field-level labels, 9,000 yield measurements, and satellite-derived inputs. \n",
    "\n",
    "Maize is classified with 83% accuracy, precision of 0.70, and recall of 0.44 and total maize cover was predicted within 4% of national statistics. Yields aggregated to districts had an RMSE of 370kg/ha (27%). It helps to create a dataset for smallholder maize classification and yield\n",
    "estimation product for sub-Saharan Africa while being accessible, low-cost, standardized, and\n",
    "observed over time; thus, being more likely to enable technology transfer and innovation for\n",
    "many uses cases and users.\n",
    "\n",
    "Below is a diagram outlining the methods used here. For the full context, we recommended you review [this pre-print paper](https://github.com/Better-Planet-Laboratory/ai4good/blob/8952fc8659b9f10fcd2dc164fac2236cd1462a1b/Notebooks/wk6/FankhauserEtal_2024_Preprint.pdf) for greater detail on context, data sources and types, and the aims/challenges of this work.\n",
    "\n",
    "![methods](https://github.com/Better-Planet-Laboratory/ai4good/blob/wk6/Notebooks/wk6/crop_yield_methods.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e2fdc-5ce3-47a4-887f-bd4771460f67",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Data Use Agreement](#Data-Use-Agreement)\n",
    "1. [Environment Setup](#Environment-Setup)\n",
    "1. [Collect and Clean data](#Data)\n",
    "1. [Feature Selection](#Feature-Selection)\n",
    "1. [Build Model](#Build-Model)\n",
    "1. [Tune Model](#Tune-Model)\n",
    "1. [Evaluate Model](#Evaluate-Model)\n",
    "1. [Assignment](#Assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47675fec-4a33-4238-8151-7d17486cbb5e",
   "metadata": {},
   "source": [
    "## Data Use Agreement\n",
    "This notebook uses proprietary data with permission from the owners. Please read the following statement and type \"I agree, Full Name\" in the cell below: \n",
    "\n",
    "> I understand that this notebook is using real data from real people with the intent of making the challenge practical and reflective of real-world challenges. I agree to not to share it with anyone else nor to attempt to identify the individuals whom the data is about. I will not make the notebook or the data public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf861f-7275-4a64-b549-cee12352b98f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the statement above and then type \"I agree\" and your full name to the print statement below\n",
    "print('I agree, Isaiah Lyons-Galante')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86aee9",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "We'll just use the CPU for this notebook since there are no images to process nor neural networks for a GPU to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736f74a163a889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:41:59.189790Z",
     "start_time": "2024-12-23T19:41:53.801185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# some general packages we'll use:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format  # limit decimal places displayed for easier viewing\n",
    "\n",
    "# additional packages needed for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, accuracy_score,\n",
    "    f1_score, precision_recall_fscore_support,\n",
    "    matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55efa9db55d432",
   "metadata": {},
   "source": [
    "## Data for Land Cover Classification\n",
    "\n",
    "Optical imagery from the Sentinel-2 satellite and auxiliary data on climate, geography, and geopolitical boundaries were used in supervised learning with crop cut data (primarily) collected in the field.  \n",
    "\n",
    "Land cover was classified into nine mutually exclusive groups: 1) maize, 2) non-maize annual crops (e.g. beans, potatoes, cassava), 3) non-maize perennial crops (e.g. bananas), 4) forest and tree cover, 5) shrub and scrubland, given the shorthand name of rangeland here, 6) structure and built environment, 7) bare ground, 8) wetland, 9) water bodies. \n",
    "\n",
    "The raw spectral output of 12 Sentinel-2 bands as well as several derived indices were summarized by agricultural season with distributional statistics (mean, standard deviation, min, max).\n",
    "\n",
    "One Acre Fund (OAF) provided the crop cut data for major crops in Rwanda while data for other land cover classes was augmented with RTI International annotated drone imagery and existing global land cover products from Dynamic World (DW), Impact Observatory (IO), and the European Space Agency (ESA).\n",
    "\n",
    "See the following table for detail on each input feature. The data provided to you includes all of these variables, but for the purposes of this tutorial we will only use a subset.  \n",
    "\n",
    "![data-fields-description](https://github.com/Better-Planet-Laboratory/ai4good/blob/wk6/Notebooks/wk6/predictor_descriptions.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea7962-a7e0-42ea-9b2e-f5dcd1198b89",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# set data configurations\n",
    "base_path = \"/kaggle/input/rwanda-crop-yields-with-sentinel-2-data/\"\n",
    "classifier_data_filename = \"training_data_for_classifier.parquet\"\n",
    "yield_data_filename = \"training_data_for_yield.csv\"\n",
    "\n",
    "print('configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5241c63d2796bb3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:42:25.371895Z",
     "start_time": "2024-12-23T19:42:25.085769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# read in the data from a parquet file\n",
    "# learn more about parquet: https://www.databricks.com/glossary/what-is-parquet\n",
    "data_raw = pd.read_parquet(os.path.join(base_path, classifier_data_filename))\n",
    "print('data imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb5468-3262-4746-8dc4-9778496f2396",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# count the sample size by land cover class\n",
    "data_raw['crop_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8166ce743cb664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:47.537850Z",
     "start_time": "2024-12-23T18:48:47.489906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# count the sample size by data source\n",
    "data_raw['crop_id'].apply(lambda s: re.sub(r\"_[0-9]+\", \"\", s)).value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37e8a67d12638d",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "In the paper, we used an empirical feature selection strategy that chose variables with the highest Mutual Information Scores that were not highly correlated with other variables. Read more about it in the methods section. \n",
    "\n",
    "Here, we will manually select variables given our expert knowledge and build a model from mean seasonal Sentinel-2 spectral information, one derived index, and topographic information from a Digital Elevation Model (DEM). Learn more about elevation, slope, and aspect [here](https://pro.arcgis.com/en/pro-app/latest/help/analysis/raster-functions/aspect-slope-function.htm).\n",
    "\n",
    "**Question:** Based on the data description table above, what other features do you think might be predictive of land cover type? What's the advantage of doing empirical feature selection versus using expert knowledge?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7d99ac3a0a959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:47.586692Z",
     "start_time": "2024-12-23T18:48:47.584494Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "id_cols = ['crop_id', 'crop_type', 'label', 'intercrop', 'season', 'sos_actual']\n",
    "s2_predictors = data_raw.columns[data_raw.columns.str.contains(\"B[0-9]*_mean$\")].tolist()\n",
    "topo_predictors = ['elevation', 'slope', 'aspect']\n",
    "predictors = s2_predictors + topo_predictors\n",
    "print(\"Predictors:\", predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2f3fe965bc6bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:47.596487Z",
     "start_time": "2024-12-23T18:48:47.592266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# subset the data to just the predictors of interest\n",
    "data = data_raw.filter(id_cols + predictors)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5d6ee6fe14e36",
   "metadata": {},
   "source": [
    "Let's compute the Normalized Difference Vegetation Index (NDVI), one of the most commonly used indices to measure vegetative health from satellite data. \n",
    "\n",
    "NDVI is a ratio between the Red and Near-Infrared bands and measures the amount of red light and near-infrared light absorbed on the ground. Typically, a plant is healthier if it absorbs more red light relative to infrared light (NDVI close to 1) while NDVI values close to 0 or negative values can indicate unhealthy vegetation or areas of non-vegetation such as bare ground, structures, or fallowed fields. Learn more about [NDVI here](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/).\n",
    "\n",
    "(Note: The mean seasonal NDVI we calculate will be different than that given in the raw data ('ndvi_mean') because the latter represents a mean of daily NDVIs whereas here we are computing mean NDVI from variables that are already averaged over the season.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cdfa734ce5076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:47.830042Z",
     "start_time": "2024-12-23T18:48:47.648272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_ndvi(b8, b4): \n",
    "    return (b8-b4) / (b8+b4)\n",
    "\n",
    "data['ndvi_mean'] = data.apply(lambda df: compute_ndvi(df['B8_mean'], df['B4_mean']), axis=1)\n",
    "\n",
    "# add to predictors list\n",
    "predictors = predictors + ['ndvi_mean']\n",
    "\n",
    "print('ndvi added to predictors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7122b921e8a52",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "\n",
    "We will use a gradient boosted decision tree to predict land cover type. Particularly, the model we will use is called [XGBoost (eXtreme Gradient Boosting)](https://www.geeksforgeeks.org/xgboost/).\n",
    "\n",
    "The advantages of this model for our prediction task are:\n",
    "* Supervised learning that can solve both regression and classification, including multinomial, problems\n",
    "* Non-parametric so does not require a lot of assumptions\n",
    "* More easily solves non-linear problems with decision boundaries\n",
    "* Hierarchical and detects interactions between features\n",
    "* Accepts heterogeneous data, robust to outliers, and requires less pre-processing (e.g. does not require feature scaling or normalization)\n",
    "\n",
    "**Question:** What are some of the limitations of XGBoost?\n",
    "\n",
    "**Your Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f030e38-774f-4aec-957e-692262a48358",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling\n",
    "The model expects that the target (i.e. outcome) is numeric and that the dataset does not contain missing data so first we must do some additional cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a4c693479293e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:52.616971Z",
     "start_time": "2024-12-23T18:48:52.606656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# create nominal outcome \n",
    "data['land_cover_class'] = pd.Categorical(  # order that matches original paper\n",
    "    data['crop_type'],\n",
    "    categories=['maize', 'nonmaize_annual', 'nonmaize_perennial', 'rangeland', 'forest', 'wetland', 'water', 'structure', 'bare'],  \n",
    "    ordered=True\n",
    "    )\n",
    "data['land_cover_class_encoded'] = data['land_cover_class'].cat.codes\n",
    "\n",
    "# remove missing data in outcome and predictors (if any)\n",
    "data.dropna(subset = ['land_cover_class_encoded'] + predictors, inplace = True)\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29b252-c547-468b-a8c8-9bb243d7e7f4",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, in order to tune and evaluate our model on independent datasets we need to split our data into training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fcad1f27a39aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:52.651085Z",
     "start_time": "2024-12-23T18:48:52.636668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# split dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data.filter(predictors), # predictors\n",
    "    data['land_cover_class_encoded'], # outcome\n",
    "    test_size=0.2, # 20% of data reserved for testing set\n",
    "    random_state=123  # seed for reproducibility \n",
    ")\n",
    "print(\"Size of training dataset: {} observations\".format(len(x_train)))\n",
    "print(\"Size of test dataset: {} observations\".format(len(x_test)))\n",
    "\n",
    "# set up data for model\n",
    "# xgb expects data to be a DMatrix object \n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)  \n",
    "dtest = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645beec79e92006",
   "metadata": {},
   "source": [
    "### Configure the Model\n",
    "\n",
    "Later we will tune our model to select the best hyperparameters, but for now use the following given hyperparameters. A full list of those available in XGBoost can be found [here](https://xgboost.readthedocs.io/en/latest/parameter.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd057865e61eecf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:52.677429Z",
     "start_time": "2024-12-23T18:48:52.675160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Choose hyperparameters for XGBoost\n",
    "param = {\n",
    "    'objective': 'multi:softmax',  # Specifies that we are solving a multiclass classification problem.\n",
    "                                   # 'multi:softmax' applies the softmax function to output class probabilities.\n",
    "                                   # Alternative: 'multi:softprob' outputs raw probabilities instead of class labels.\n",
    "\n",
    "    'num_class': data['land_cover_class_encoded'].nunique(),  \n",
    "                                   # Number of unique classes in the target variable.\n",
    "                                   # Required for multiclass classification.\n",
    "\n",
    "    'eval_metric': ['mlogloss', 'merror'],  \n",
    "                                   # Evaluation metrics to assess model performance:\n",
    "                                   # 'mlogloss': Multinomial negative log-likelihood (cross-entropy loss for multiple classes).\n",
    "                                   # 'merror': Multiclass classification error rate (fraction of incorrect predictions).\n",
    "\n",
    "    'subsample': 0.5,              # Fraction of training data used for training each tree (stochastic training).\n",
    "                                   # Reducing it below 1.0 helps prevent overfitting by introducing randomness.\n",
    "                                   # Values typically range from 0.5 to 1.0.\n",
    "\n",
    "    'sampling_method': 'uniform',  # Specifies how samples are selected when 'subsample' is used.\n",
    "                                   # 'uniform': Randomly selects samples with equal probability.\n",
    "                                   # 'gradient_based': Prioritizes more informative samples (less common for classification tasks).\n",
    "\n",
    "    'eta': 0.3,                    # Learning rate (also known as \"shrinkage factor\").\n",
    "                                   # Controls the contribution of each tree to the final prediction.\n",
    "                                   # Lower values (e.g., 0.1) slow down learning, requiring more trees to converge.\n",
    "                                   # Higher values (e.g., 0.3) speed up learning but risk overfitting.\n",
    "\n",
    "    'max_depth': 6,                # Maximum depth of each decision tree.\n",
    "                                   # Higher values allow more complex patterns to be learned but increase overfitting risk.\n",
    "                                   # Lower values help with generalization but may miss complex patterns.\n",
    "                                   # Typical values: 3–10.\n",
    "\n",
    "    'min_child_weight': 1,         # Minimum sum of instance weights required in a child node.\n",
    "                                   # Higher values make trees more conservative by requiring larger splits.\n",
    "                                   # Lower values allow smaller child nodes, making trees more complex.\n",
    "                                   # Default is 1.\n",
    "\n",
    "    'seed': 529                    # Random seed for reproducibility.\n",
    "                                   # Ensures that results remain consistent when running the model multiple times.\n",
    "}\n",
    "\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde4437-f528-4566-be5c-7d8921dcf38a",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58161ee766a36a89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:58.124707Z",
     "start_time": "2024-12-23T18:48:52.687648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "classifier = xgb.train(\n",
    "    params=param, dtrain=dtrain, \n",
    "    num_boost_round=1000,  # number of boosting iterations, \n",
    "    early_stopping_rounds=100,  # helps control overfitting\n",
    "    evals=[(dtrain, 'train'), (dtest, 'eval')], verbose_eval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45fa3081a64583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:58.136961Z",
     "start_time": "2024-12-23T18:48:58.134856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# output performance on testing set, considering early stopping \n",
    "print(\"Lowest error was {:.2f} after {} rounds\".format(\n",
    "    classifier.best_score,\n",
    "    classifier.best_iteration+1\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a153871b99ad1",
   "metadata": {},
   "source": [
    "## Tune Model\n",
    "\n",
    "The architecture of the gradient boosted tree, defined by hyperparameters, is often specific to the data and target. Choosing the right parameter values can have a big impact on performance. Hyperparameter tuning is the process by which you run a model with different sets of parameter values and choose the best settings empirically. \n",
    "\n",
    "We will look at two parameters that control model complexity and overfitting: maximum tree depth and learning rate. \n",
    "* `max_depth`: Deeper trees increase complexity and contain more information about the interactions between predictors, but may increase noise.\n",
    "* `eta`: The shrinkage parameter is a regularization parameter applied to each tree to control for overfitting; with a sufficient number of boosting iterations, smaller values are likely to give more accurate results.\n",
    "\n",
    "The number of boosting iterations, or trees, (`num_boost_round`) is also an important hyperparameter and inversely proportional to the learning rate: as the learning rate decreases, the number of trees must increase  for the model to converge as the model is learning less information at each step. Here, we capture the optimal number of iterations with early stopping, which stops training if performance has not improved for a specified number of rounds (here, n=100). \n",
    "\n",
    "We will evaluate the relative performance of each setting with 5-fold cross-validation using the training set alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dc40c2f3ff148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:48:58.148711Z",
     "start_time": "2024-12-23T18:48:58.146632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, eta)\n",
    "    for max_depth in [4, 6, 8]\n",
    "    for eta in [0.2, 0.1, 0.01]\n",
    "]\n",
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c45dea1efd09b",
   "metadata": {},
   "source": [
    "⚠️ Note: This next code block will take some time to run. On an Apple M1 Pro (3.2 GHz 8-core) processor with 16 GB RAM, it took 15-20 minutes to complete. On an Apple M4 Pro processor with 24GB RAM (14-core), it still took 15 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679be6db3810aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:06:29.493628Z",
     "start_time": "2024-12-23T18:48:58.158423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run hyperparameter search \n",
    "min_mlogloss = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for max_depth, eta in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, eta={}\".format(max_depth, eta))\n",
    "    \n",
    "    # Update parameters\n",
    "    param['max_depth'] = max_depth\n",
    "    param['eta'] = eta\n",
    "    \n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params=param, \n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=3000,\n",
    "        early_stopping_rounds=100,\n",
    "        nfold=5\n",
    "    )\n",
    "    \n",
    "    # Update best performance\n",
    "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
    "    n_boost = cv_results['test-mlogloss-mean'].argmin()\n",
    "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, n_boost))\n",
    "    if mean_mlogloss < min_mlogloss:\n",
    "        min_mlogloss = mean_mlogloss\n",
    "        best_params = {'max_depth': max_depth, 'eta': eta, 'num_boost_round': int(n_boost+1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bca48e4f6bc7a8",
   "metadata": {},
   "source": [
    "When max depth was equal to 4 and learning rate was equal to 0.01 it looks like the error never reached a saturation point up to 3000 boosting iterations, so we may consider increasing the numbering of rounds, but we should also keep in mind that any marginal gain in performance may not be worth the computation time required. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4fc5ec24026cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:06:29.520284Z",
     "start_time": "2024-12-23T19:06:29.518162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a99abb6d7cf506",
   "metadata": {},
   "source": [
    "Let's train a model with these settings on the full training set (i.e. no cross-validation) and then evaluate performance on the test set. This takes about 1 minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7547cf0a986fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:07:24.678754Z",
     "start_time": "2024-12-23T19:06:29.644262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# update param dictionary with best settings\n",
    "param.update({k:v for k,v in best_params.items() if k in param.keys()})\n",
    "\n",
    "# final model\n",
    "best_classifier = xgb.train(params=param, dtrain=dtrain, \n",
    "                            num_boost_round=best_params['num_boost_round'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2b24f2a7f5177",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4932425a3bd8f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:07:25.109408Z",
     "start_time": "2024-12-23T19:07:24.691425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get class predictions on test set\n",
    "y_pred = best_classifier.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632107ad4f43bcd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:07:25.135021Z",
     "start_time": "2024-12-23T19:07:25.120838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "land_cover_classes = data['land_cover_class'].cat.categories.tolist()\n",
    "\n",
    "cm_pd = pd.DataFrame(cm, index=land_cover_classes, columns=land_cover_classes).rename_axis(\"actual/pred\")\n",
    "cm_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b4e220ed0dfe9",
   "metadata": {},
   "source": [
    "**Question:** What is maize cover most commonly mistaken as? Which categories is it most easily differentiated from?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c8e98e9eee572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:07:25.171458Z",
     "start_time": "2024-12-23T19:07:25.162261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# overall performance considering all land cover types\n",
    "overall_eval = pd.DataFrame(data=np.array([    \n",
    "    balanced_accuracy_score(y_test, y_pred),\n",
    "    f1_score(y_test, y_pred, average='weighted'),\n",
    "    matthews_corrcoef(y_test, y_pred)]),\n",
    "    columns = [\"overall\"], index= ['balanced_accuracy', 'weighted_f1_score', 'mcc'])\n",
    "overall_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b903ccae358809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:07:25.214617Z",
     "start_time": "2024-12-23T19:07:25.204211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# performance by land cover class\n",
    "eval_by_class = pd.DataFrame(data = np.concatenate([\n",
    "    np.expand_dims(np.array([accuracy_score(np.array(y_test) == i, np.array(y_pred) == i) for i, name in enumerate(land_cover_classes)]), 0),\n",
    "    np.vstack(precision_recall_fscore_support(y_test, y_pred, average=None))], axis=0),\n",
    "             columns = land_cover_classes,\n",
    "             index = ['accuracy', 'precision', 'recall', 'f1_score', 'n'])\n",
    "eval_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e30840f3f66918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T19:07:25.507343Z",
     "start_time": "2024-12-23T19:07:25.276182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# variable importance plot\n",
    "xgb.plot_importance(best_classifier)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efcb2aa2a5dadbe",
   "metadata": {},
   "source": [
    "**Question:** Which predictors were most informative? Which spectral information was most informative? Can you imagine why these features would be most predictive of land cover class? Keep in mind this is for all land cover classes, not just crops/vegetation. \n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6e68c5151df9d",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "**This week, do not make your notebook public. Instead, add Isaiah as a collaborator @isaiahlg.** Still submit the link to your notebook on Discord.\n",
    "\n",
    "In the code above, we used XGBoost for classifying land cover based on a set of input variables. Instead of classification, update this workflow to predict maize yields, a continuous outcome measure. Use the maize yield data provided in `training_data_for_yield.csv`. Report on your results in the same format as the pre-print paper:\n",
    "\n",
    "> The predictions of maize yields were different from observed yields on average by about 1650 kg/ha, or 44% of the mean in observed yields (RMSE). The RMSE gives greater weight to outliers than the mean absolute error (MAE); when considering residual error irrespective of magnitude, average error in yield was 1300 kg/ha. The correlation between observed and predicted yield was moderate in the test set, with a value of 0.52 (and 0.65 in training).\n",
    "\n",
    "That is, report on the RMSE, MAE, and correlation for both your train and test datasets. No need to focus a lot on hyperparameter tuning and optimization, save that for the bonus assignment. We recommend writing new code instead of modifying the code above so that you can have both a classification and yield pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a179b5-cb39-4430-8ed0-5ec95236a153",
   "metadata": {},
   "source": [
    "## Bonus Assignment\n",
    "\n",
    "The results reported in the pre-print paper are as follows: \n",
    "> Maize was classified with 83% accuracy, precision of 0.70, and recall of 0.44 and total maize cover was predicted within 4% of national statistics.\n",
    "> The predictions of maize yields were different from observed yields on average by about 1650 kg/ha, or 44% of the mean in observed yields (RMSE).\n",
    "\n",
    "Improve on either or both of the results for maize cover classification and yield prediction at the 10-meter pixel. You are encouraged to choose from any of the input variables available and/or to add in any additional open-source datasets that you think would be relevant to the prediction task. You can also continue to fine-tune the hyperparameters, use a different loss function (maybe one that optimizes performance for the maize class?) You can even try a different model entirely. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563134d",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Thank you to [Katie Fankhauser](https://www.linkedin.com/in/katie-fankhauser/) for putting together this tutorial based on her ongoing research. Thank you to [One Acre Fund](https://oneacrefund.org/) for providing the crop data and yield measurements. This work is a product of the [Better Planet Laboratory](https://betterplanetlab.com/) at the University of Colorado Boulder."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6685237,
     "sourceId": 10775214,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "crop_classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
