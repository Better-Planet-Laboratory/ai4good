{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{}},{"cell_type":"markdown","source":"The aim of this notebook is to get you set up in the Kaggle environment. We'll read and write some data, run a functions, create a plot so you are good to go for the coming weeks.Below is the chunk that loads on a fresh kaggle notebook, it includes some details on the resources available. You can explore more by selecting View (at top right of page) -> Show sidebar. In here (to right of page) are some clickable settings for the notebook, including compute resources used, location of persistance storage and more.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:05:19.181848Z","iopub.execute_input":"2025-01-02T21:05:19.182082Z","iopub.status.idle":"2025-01-02T21:05:19.561001Z","shell.execute_reply.started":"2025-01-02T21:05:19.182040Z","shell.execute_reply":"2025-01-02T21:05:19.560209Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test read, write ","metadata":{}},{"cell_type":"markdown","source":"First let's grab some images.. using following api https://pypi.org/project/bing-image-downloader/","metadata":{}},{"cell_type":"code","source":"pip install bing-image-downloader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:05:19.561749Z","iopub.execute_input":"2025-01-02T21:05:19.562099Z","iopub.status.idle":"2025-01-02T21:05:24.284914Z","shell.execute_reply.started":"2025-01-02T21:05:19.562048Z","shell.execute_reply":"2025-01-02T21:05:24.283865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom bing_image_downloader import downloader\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:05:24.286073Z","iopub.execute_input":"2025-01-02T21:05:24.286361Z","iopub.status.idle":"2025-01-02T21:05:24.294835Z","shell.execute_reply.started":"2025-01-02T21:05:24.286338Z","shell.execute_reply":"2025-01-02T21:05:24.294207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"downloader.download(\"child hungry\", limit=5, output_dir=\"images\", adult_filter_off=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:05:24.295744Z","iopub.execute_input":"2025-01-02T21:05:24.295994Z","iopub.status.idle":"2025-01-02T21:06:32.509259Z","shell.execute_reply.started":"2025-01-02T21:05:24.295965Z","shell.execute_reply":"2025-01-02T21:06:32.508387Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%ls /kaggle/working/images/'child hungry'/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:32.510212Z","iopub.execute_input":"2025-01-02T21:06:32.510537Z","iopub.status.idle":"2025-01-02T21:06:32.632786Z","shell.execute_reply.started":"2025-01-02T21:06:32.510512Z","shell.execute_reply":"2025-01-02T21:06:32.631677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef load_and_display_images(image_dir):\n    \"\"\"\n    Load images from a directory, store them in a list, \n    and display their thumbnails with names.\n\n    Parameters:\n    image_dir (str): Directory containing image files.\n    \n    Returns:\n    images (list): List of PIL image objects.\n    image_names (list): List of image filenames.\n    \"\"\"\n    # 1. Load all images and store them in a list\n    images = []\n    image_names = []\n    \n    for filename in os.listdir(image_dir):\n        if filename.lower().endswith(('jpg', 'jpeg')):\n            image_path = os.path.join(image_dir, filename)\n            images.append(Image.open(image_path))\n            image_names.append(filename)\n\n    # 2. Plot thumbnails of the images with their names\n    fig, axes = plt.subplots(1, len(images), figsize=(15, 5))\n    if len(images) == 1:\n        axes = [axes]  # Ensure axes is iterable when there's only 1 image\n\n    for ax, img, name in zip(axes, images, image_names):\n        ax.imshow(img.resize((128, 128)))  # Resize for thumbnail\n        ax.set_title(name, fontsize=8)\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    return images, image_names\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:32.635191Z","iopub.execute_input":"2025-01-02T21:06:32.635463Z","iopub.status.idle":"2025-01-02T21:06:32.642402Z","shell.execute_reply.started":"2025-01-02T21:06:32.635441Z","shell.execute_reply":"2025-01-02T21:06:32.641474Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage\nimage_dir = '/kaggle/working/images/child hungry/'\nimages, image_names = load_and_display_images(image_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:32.643459Z","iopub.execute_input":"2025-01-02T21:06:32.643679Z","iopub.status.idle":"2025-01-02T21:06:33.640014Z","shell.execute_reply.started":"2025-01-02T21:06:32.643660Z","shell.execute_reply":"2025-01-02T21:06:33.639111Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"lets grab some other, clearly different, images.","metadata":{}},{"cell_type":"code","source":"downloader.download(\"child happy\", limit=5, output_dir=\"images\", adult_filter_off=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:33.641035Z","iopub.execute_input":"2025-01-02T21:06:33.641375Z","iopub.status.idle":"2025-01-02T21:06:37.339616Z","shell.execute_reply.started":"2025-01-02T21:06:33.641345Z","shell.execute_reply":"2025-01-02T21:06:37.338746Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's put these in the same directory to mix things up a bit..","metadata":{}},{"cell_type":"code","source":"source_dir = '/kaggle/working/images/child happy/'\ndestination_dir = '/kaggle/working/images/child hungry/'\n\nos.makedirs(destination_dir, exist_ok=True)\n\nfor index, file in enumerate(os.listdir(source_dir)):\n    src = os.path.join(source_dir, file)\n    if os.path.isfile(src):\n        shutil.move(src, os.path.join(destination_dir, f\"image{index}{os.path.splitext(file)[1]}\"))\n\n%ls /kaggle/working/images/'child happy'/\n!rm -r /kaggle/working/images/'child happy'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:37.340700Z","iopub.execute_input":"2025-01-02T21:06:37.341041Z","iopub.status.idle":"2025-01-02T21:06:37.581996Z","shell.execute_reply.started":"2025-01-02T21:06:37.341009Z","shell.execute_reply":"2025-01-02T21:06:37.580790Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now our directory has some mixed up happy and hungry images mixed in..","metadata":{}},{"cell_type":"code","source":"%ls /kaggle/working/images/'child hungry'/\n\nimages, image_names = load_and_display_images('/kaggle/working/images/child hungry/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:37.583242Z","iopub.execute_input":"2025-01-02T21:06:37.583515Z","iopub.status.idle":"2025-01-02T21:06:39.195932Z","shell.execute_reply.started":"2025-01-02T21:06:37.583491Z","shell.execute_reply":"2025-01-02T21:06:39.194888Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Going in blind","metadata":{}},{"cell_type":"markdown","source":"Suppose we didn't obtain these images from the internet. Imagine we had millions of pictures to evaluate not just these. How might we assess what we have? One option is to try some off-the-shelf models to help extract text based meaning from the images. And then see if we can enumerate from there. We'll be using the Hugging Face APIs for this... https://huggingface.co/tasks/image-text-to-text","metadata":{}},{"cell_type":"code","source":"from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:39.197043Z","iopub.execute_input":"2025-01-02T21:06:39.197389Z","iopub.status.idle":"2025-01-02T21:06:52.272879Z","shell.execute_reply.started":"2025-01-02T21:06:39.197357Z","shell.execute_reply":"2025-01-02T21:06:52.271905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now enable the GPU on the right hand panel and check it is working. If CPU is returned try again. Note, you only have 30 hours per week so make sure you use your time where needed and for the computation only.","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:06:52.273967Z","iopub.execute_input":"2025-01-02T21:06:52.274596Z","iopub.status.idle":"2025-01-02T21:06:52.329429Z","shell.execute_reply.started":"2025-01-02T21:06:52.274569Z","shell.execute_reply":"2025-01-02T21:06:52.328463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Below is a function that takes a random selection of images and some text input and outputs some text for each image. It uses a small model ~1.7GB (0.B params) so is not massively accurate. At the same time the individual descriptions are interesting. Don't worry too much about the technical details right now (this is just a demo set up notebook, and in future sessions we'll pre-empt any work done in notebooks in session with conceptual and theoretical backgrounds, but if interested here is the background paper https://arxiv.org/pdf/2407.07895).","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\n\n# Load the model and processor once to avoid reloading them for each image\nmodel_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\ndevice = \"cuda:0\"  # Assuming you're using GPU\n\n# Load model and processor\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id, \n    low_cpu_mem_usage=True\n).to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n# Function to process and generate response for a given image and prompt\ndef generate_response(image_path, prompt):\n    # Load image from the path\n    image = Image.open(image_path).convert(\"RGB\")\n\n    # Define a chat history and use `apply_chat_template` to get the correctly formatted prompt\n    conversation = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image\"},\n            ],\n        },\n    ]\n\n    # Apply chat template to the conversation\n    formatted_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n\n    # Process the inputs (text and image)\n    inputs = processor(formatted_prompt, image, return_tensors=\"pt\").to(device)\n\n    # Generate the output from the model\n    output = model.generate(**inputs, max_new_tokens=100)\n\n    # Decode and return the response\n    return processor.decode(output[0], skip_special_tokens=True), image\n\n# Function to randomly select a specified number of images from a directory and process them with a given prompt\ndef process_multiple_random_images_from_directory(directory, prompt, num_images=5):\n    # List all image files in the directory (assuming image files are .jpg, .jpeg, or .png)\n    image_files = [f for f in os.listdir(directory) if f.lower().endswith(('jpg', 'jpeg', 'png'))]\n    \n    # If no image files are found, return\n    if not image_files:\n        print(\"No image files found in the directory.\")\n        return\n\n    # If the requested number of images exceeds the available ones, adjust\n    num_images = min(num_images, len(image_files))\n    \n    # Randomly select the specified number of images\n    selected_image_files = random.sample(image_files, num_images)\n    \n    # Process each randomly selected image\n    for selected_image_file in selected_image_files:\n        image_path = os.path.join(directory, selected_image_file)\n        print(f\"Processing randomly selected image: {selected_image_file}\")\n        response, image = generate_response(image_path, prompt)\n        \n        # Display the image and the response\n        display_image_with_response(image, response, selected_image_file)\n\n# Function to display an image and its corresponding model response\ndef display_image_with_response(image, response, image_file):\n    # Create a figure and axes\n    plt.figure(figsize=(8, 8))\n    \n    # Display the image\n    plt.imshow(image)\n    plt.axis('off')  # Hide axes\n    \n    # Display the response as a title\n    plt.title(f\"Response for {image_file}:\\n{response}\", fontsize=12)\n    \n    # Show the image with response\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:14:28.668920Z","iopub.execute_input":"2025-01-02T22:14:28.669240Z","iopub.status.idle":"2025-01-02T22:14:33.101620Z","shell.execute_reply.started":"2025-01-02T22:14:28.669215Z","shell.execute_reply":"2025-01-02T22:14:33.100898Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"directory = \"/kaggle/working/images/child hungry/\"\nprompt = \"Describe the contents of this image.\"\n\n# Example call to process 3 random images from a given directory\nprocess_multiple_random_images_from_directory(directory, prompt, num_images=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T22:14:33.102606Z","iopub.execute_input":"2025-01-02T22:14:33.102820Z","iopub.status.idle":"2025-01-02T22:14:39.293386Z","shell.execute_reply.started":"2025-01-02T22:14:33.102802Z","shell.execute_reply":"2025-01-02T22:14:39.292371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"markdown","source":"Below we check out the ability of the model to capture sentiment. Not so good! Clearly needs some fine tuning.","metadata":{}},{"cell_type":"code","source":"directory = \"/kaggle/working/images/child hungry/\"\nprompt = \"Is this image a happy scene or not?\"\n\n# Example call to process 3 random images from a given directory\nprocess_multiple_random_images_from_directory(directory, prompt, num_images=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"One interesting thing about the model above is that it can aparently used to query multiple images at once. This might be useful for assessing the contents aross the set of images we have... might be worth exploring this later, if it is unable to do sentiment on single images I would not trust it on multiple comparisons!","metadata":{}},{"cell_type":"markdown","source":"## Clean up","metadata":{}},{"cell_type":"markdown","source":"If you happen to want to clean up your working directory you can uncomment the following.","metadata":{}},{"cell_type":"code","source":"#!rm -r /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T21:07:43.100610Z","iopub.status.idle":"2025-01-02T21:07:43.100862Z","shell.execute_reply":"2025-01-02T21:07:43.100763Z"}},"outputs":[],"execution_count":null}]}